{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_xhvUO10XD6"
      },
      "source": [
        "# Hugging Face の利用方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Sx06lGrg3K"
      },
      "source": [
        "## Hugging Face とは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-S6ghDZpmSG"
      },
      "source": [
        "Hugging Face とは主にトランスフォーマーに関連する深層学習モデルやそれに付随する技術の提供を行っているサービスです．トランスフォーマー（のエンコーダーとデコーダー）はニューラルネットワーク等からなる，様々なタイプのデータを処理できる機械学習モデルです．トランスフォーマーは色々な問題を解決するために少しずつ違う構造を持ちますが，そのようなものをまとめてコマンド一発で利用可能にしてくれているのが Hugging Face です．また，自然言語処理の分野で扱うデータのサイズはとても大きい場合があるのですが，どこかの誰かがあらかじめ何かのデータを利用して学習済みのデータを提供してくれていることがあり，そのような学習済みモデルも簡単に利用できるように整備してくれています．Hugging Face を利用すれば，特に自然言語処理に関する問題を簡単に解けるようになるため紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isuh1_iiygT0"
      },
      "source": [
        "### できること"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtIMcWPodXRj"
      },
      "source": [
        "Hugging Face で扱うことができるタスクは以下に示すものがあります．これ以外にもありますが自然言語処理に関する代表的なタスクを抽出しました．括弧内の文字列は実際に Hugging Face を利用する際に指定するオプションです（後で利用します．）．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRGjF8pHdtO1"
      },
      "source": [
        "    \n",
        "\n",
        "*   感情分析（`sentiment-analysis`）：入力した文章が有する感情を予測\n",
        "*   特徴抽出（`feature-extraction`）：入力した文章をその特徴を示すベクトルに変換\n",
        "*   穴埋め（`fill-mask`）：文章中のマスクされた単語を予測\n",
        "*   固有表現抽出（`ner`）：入力した文章中の固有表現（名前とか場所とか）にラベルをつける\n",
        "*   質問応答（`question-answering`）：質問文とその答えが含まれる何らかの説明文を入力として解答文を生成\n",
        "*   要約（`summarization`）：入力した文章を要約\n",
        "*   文章生成（`text-generation`）：文章を入力にして，その文章に続く文章を生成\n",
        "*   翻訳（`translation`）：文章を他の言語に翻訳\n",
        "*   ゼロショット文章分類（`zero-shot-classification`）：文章とそれが属する可能性があるいくつかのカテゴリを入力にしてその文章をひとつのカテゴリに分類\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3mzuNEjy5to"
      },
      "source": [
        "### Hugging Face Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7fpi958zLiS"
      },
      "source": [
        "Hugging Face の利用方法は以下のウェブサイト，Hugging Face Course の Transformer models の部分を読めば大体のことが把握できると思います．\n",
        "\n",
        "https://huggingface.co/course/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hezey89MynbX"
      },
      "source": [
        "### インストール"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG4DcIBO42U_"
      },
      "source": [
        "Hugging Face（のトランスフォーマー）は TensorFlow または PyTorch とあわせて利用可能なライブラリです．本来は TensorFlow か PyTorch をあらかじめインストールする必要があります．グーグルコラボラトリーには既に TensorFlow がインストールされているため不要です．以下のように `transformers` のみをインストールすれば利用可能です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp0yFo3Se8PB"
      },
      "outputs": [],
      "source": [
        "! pip3 install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYrJO84QpmSM"
      },
      "source": [
        "```{attention}\n",
        "このコンテンツの実行にグーグルコラボラトリーを使っておらず，自身の環境で Anaconda 等を利用している場合は気をつけてください．例えば，Anaconda を利用しているのであれば ` conda install -c huggingface transformers ` のようなコマンドでインストールした方が良いです．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RlLdfs5ynbW"
      },
      "source": [
        "## 基本的な使い方"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZpHl3AlpmSN"
      },
      "source": [
        "とても簡単に自然言語処理を実現することができる利用方法を紹介します．世界最高性能を求めたいとかでないなら，ここで紹介する方法を利用して様々なことを達成できると思います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saOu-Onk5d-w"
      },
      "source": [
        "### 感情分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RY-XjT5d-w"
      },
      "source": [
        "最も簡単な `tranformers` の利用方法は以下のようになると思います．`pipeline` を読み込んで，そこに取り組みたいタスク（`sentiment-analysis`）を指定します．初回の起動の際には事前学習済みモデルがダウンロードされるため時間がかかります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QgsbGyfBA3"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "    text = \"I have a pen.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE4KbHIJHDFk"
      },
      "source": [
        "入力した文章がポジティブな文章なのかネガティブな文章なのかを分類できます．ここでは1個の文章を入力しましたが，以下のように2個以上の文章も入力可能です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVrVwjvFHVax"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "    litext = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        "    result = classifier(litext)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEBdCL6KcIo8"
      },
      "source": [
        "### 特徴抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5N2rUuWcIo_"
      },
      "source": [
        "文字列の特徴を抽出して何らかの分散表現にしたい場合は `feature-extraction` を指定します．文字列の長さに依存した配列が出力されますが，同じ長さのベクトルにしたい場合は配列長に渡って要素の値を足し算すること等で特徴量を得ることができます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbEy3hgncIpA"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    converter = pipeline(\"feature-extraction\")\n",
        "    text = \"We are very happy to introduce pipeline to the transformers repository.\"\n",
        "    result = converter(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIhMpetYd9SW"
      },
      "source": [
        "```{hint}\n",
        "例えば，BERT を使うと各トークンが768次元のベクトルでトークン数個からなる要素のベクトルが出力されます．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjztlAuqHzky"
      },
      "source": [
        "### ゼロショット文章分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M6_h785Hzky"
      },
      "source": [
        "ゼロショット文章分類は以下のように利用します．ゼロショットとは訓練中に一度も出現しなかったクラスの分類タスクです．ラベルが未定義の問題に利用することができます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAXUlTWnIEVE"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    classifier = pipeline(\"zero-shot-classification\")\n",
        "    text = \"This is a course about the Transformers library\",\n",
        "    lilabel = [\"education\", \"politics\", \"business\"]\n",
        "    result = classifier(text, candidate_labels = lilabel)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfsZyVRAIbZo"
      },
      "source": [
        "```{note}\n",
        "ゼロショット分類では，例えば，馬を入力にして猫とか犬とかのラベルそのものを予測されるのではなくて猫および犬ベクトルを予測させどちらに近いかを予測させることができます．そろそろマシンパワー的にきついかもしれませんね．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eW9CmBX5p2K"
      },
      "source": [
        "### 文章生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgHhNYYjiTvR"
      },
      "source": [
        "文章の生成は以下のように行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ean4ulzOIy54"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    generator = pipeline(\"text-generation\")\n",
        "    text = \"In this course, we will teach you how to\"\n",
        "    result = generator(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EIb-OP5xoz"
      },
      "source": [
        "### 穴埋め"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWPChdEo5l_x"
      },
      "source": [
        "穴埋めは以下のように行います．以下のコードでは可能性が高いものふたつを表示させます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4MA5Uw6JYiw"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    unmasker = pipeline(\"fill-mask\")\n",
        "    text = \"This course will teach you all about <mask> models.\"\n",
        "    result = unmasker(text, top_k=2)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5morCbD95ytz"
      },
      "source": [
        "### 固有表現抽出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDj3vKYP5yt0"
      },
      "source": [
        "固有表現抽出は以下のように行います．結果の `PER` は人名，`ORG` は組織名，`LOC` は地名です．それらの固有表現の文書中における位置も `start` と `end` で示されています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKnlUcasJeVW"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    ner = pipeline(\"ner\", grouped_entities=True)\n",
        "    text = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "    result = ner(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtflLBrq5zhl"
      },
      "source": [
        "### 質問応答"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRqwDbBK5zhl"
      },
      "source": [
        "SQuAD のような機械学習コンテストで行われる質問応答は質問だけを入力にして何かを出力する問題ではありません．質問文と何らかの説明文を入力にして解答を出力される問題です．以下のように利用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJdXGoVRJfG_"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    question_answerer = pipeline(\"question-answering\")\n",
        "    question_text = \"Where do I work?\"\n",
        "    explanation = \"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
        "    result = question_answerer(question = question_text, context = explanation)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GL03sOJj2z"
      },
      "source": [
        "### 要約"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GgSz_UKJj2z"
      },
      "source": [
        "文章の要約は以下のように行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BarS-2rN0u7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    text = \"\"\"\n",
        "        America has changed dramatically during recent years. Not only has the number of \n",
        "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
        "    the premier American universities engineering curricula now concentrate on \n",
        "    and encourage largely the study of engineering science. As a result, there \n",
        "    are declining offerings in engineering subjects dealing with infrastructure, \n",
        "    the environment, and related issues, and greater concentration on high \n",
        "    technology subjects, largely supporting increasingly complex scientific \n",
        "    developments. While the latter is important, it should not be at the expense \n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other \n",
        "    industrial countries in Europe and Asia, continue to encourage and advance \n",
        "    the teaching of engineering. Both China and India, respectively, graduate \n",
        "    six and eight times as many traditional engineers as does the United States. \n",
        "    Other industrial countries at minimum maintain their output, while America \n",
        "    suffers an increasingly serious decline in the number of engineering graduates \n",
        "    and a lack of well-educated engineers.\n",
        "    \"\"\"\n",
        "    result = summarizer(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_diMPDJ8Je"
      },
      "source": [
        "### 翻訳"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRwU4giyJ8Jf"
      },
      "source": [
        "翻訳はこれまでと少しだけ指定方法が異なります．以下のように `translation_XX_to_YY` としなければなりません．ここでは，英語からフランス語への翻訳を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Iv7oI1PJ8Jf"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    translator = pipeline(\"translation_en_to_fr\")\n",
        "    text = \"This course is produced by Hugging Face.\"\n",
        "    result = translator(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqwWY1c0O02R"
      },
      "source": [
        "## 応用的な使い方"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjsr5ACupmST"
      },
      "source": [
        "これまでに利用したものとは異なるモデルを利用したいとか，自身が持っているデータセットにより適合させたいとかの応用的な利用方法を紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSKzyoA1O02W"
      },
      "source": [
        "### 他のモデルの利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G316vauoido7"
      },
      "source": [
        "これまでに，Hugging Face が自動でダウンロードしてくれたデフォルトの事前学習済みモデルを利用した予測を行いましたが，そうでなくて，モデルを指定することもできます．以下のページをご覧ください．Model Hub と言います．\n",
        "\n",
        "https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
        "\n",
        "\n",
        "この Model Hub の Tasks というところでタグを選択できます．例えば，Text Generation の `distilgpt2` を利用するには以下のように書きます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEduyYGdPhf0"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        " \n",
        "def main():\n",
        "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "    text = \"In this course, we will teach you how to\"\n",
        "    result = generator(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z4xVexiVKfN"
      },
      "source": [
        "### 日本語の解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wK2cF2YVKfR"
      },
      "source": [
        "日本語も扱うことができます．ここでは，日本語で書かれた文章の感情分析を行います．最初に，必要なライブラリをダウンロードしてインストールします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qknFQF2XVe9e"
      },
      "outputs": [],
      "source": [
        "! pip install fugashi\n",
        "! pip install ipadic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-odvcOynXvTH"
      },
      "source": [
        "```{note}\n",
        "これをインストールしないで使ったらインストールしろってメッセージが出たからインストールしました．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Zjx7FQpmSU"
      },
      "source": [
        "```{attention}\n",
        "グーグルコラボラトリーでのインストール方法です．\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLSh2N3ZX6r7"
      },
      "source": [
        "Model Hub で調べたら以下のような感情分析のモデルが公開されていたので，それを使います．トークナイザーとは文章をトークン化（単語化して数字を割り当てます）してくれるものです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGPJoEbVVKfR"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline\n",
        "\n",
        "def main():\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\", tokenizer=\"daigo/bert-base-japanese-sentiment\")\n",
        "    text = \"みんながマリオのチョコエッグツイートをしていく中，未だにひとつも買えなくて焦る…．\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A99SjtkzYGhL"
      },
      "source": [
        "以下のようにモデルやトークナイザーは明示的に書くこともできます．ここでは，東北大学の乾研が公開している日本語版BERTを利用してみました．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AicM55hXPPS"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def main():\n",
        "    mytokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "    unmasker = pipeline(\"fill-mask\", model=\"cl-tohoku/bert-base-japanese-whole-word-masking\", tokenizer=mytokenizer)\n",
        "    text = \"みんなが[MASK]のチョコエッグツイートをしていく中，未だにひとつも買えなくて焦る…．\"\n",
        "    result = unmasker(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UNdPpS3qMjv"
      },
      "source": [
        "### モデルの設定変更"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt3UNnsZqMjz"
      },
      "source": [
        "上で紹介したのと同様に，モデルとトークナイザーをそれぞれ，`TFAutoModelForSequenceClassification` と `AutoTokenizer` で読み込むことができます．以下の通りです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riYXQjfbqMjz"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def main():\n",
        "    mymodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    mytokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
        "    text = \"I do not have a pen but I am happy.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BnWxkMStaGv"
      },
      "source": [
        "この際に `Auto` を使わずにあらかじめ用意されていいるクラスを明示的に指定することもできます．この場合，以下のように書きます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oyaw5dTte0a"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
        "\n",
        "def main():\n",
        "    mymodel = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    mytokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
        "    text = \"I do not have a pen but I am happy.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DAQNqrtnxe"
      },
      "source": [
        "さらにモデルをカスタマイズすることもできます．ここでは，`DistilBert` の構造を変えてしまっているので，全く性能が出ていません．学習をし直す必要がありそうです．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niN8Oniptu20"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "\n",
        "def main():\n",
        "    myconfig = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
        "    mymodel = TFDistilBertForSequenceClassification(myconfig)\n",
        "    mytokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
        "    text = \"I do not have a pen but I am happy.\"\n",
        "    result = classifier(text)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQwtexEFY0jp"
      },
      "source": [
        "### ファインチューニング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i__HcsHYY0ju"
      },
      "source": [
        "事前学習モデルを自分の解きたい問題に合わせてファインチューニングすることができます．ここでは，インターネット上の映画レビューのデータセット IMDb に対してモデルのファインチューニングを行います．最初に Hugging Face が提供してくれているデータセットを利用するためのモジュールをインストールします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9CAu2TXwo5a"
      },
      "outputs": [],
      "source": [
        "! pip3 install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spMlnpdgwyzM"
      },
      "source": [
        "中身を確認します．映画に関するレビューが含まれています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5DEA0mrwxdl"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    print(imdb[\"train\"][0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak6bsiQbxTjd"
      },
      "source": [
        "このデータセットをトークナイズします．以下のようなコードを追加します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyITun0txTjd"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    print(tokenized_imdb)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMbsATli53L7"
      },
      "source": [
        "データセット中のデータをパディングします．データセット中の最大の長さのデータに合わせてパディングしても良いのですが，それだと非効率的なので，バッチ毎にパディングする方法を行います．ダイナミックパディングと呼ばれる方法です．以下のようにします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKnzh27_6P6u"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2dQT4Zv8iEC"
      },
      "source": [
        "ファインチューニング前のモデルを呼び出して，テストデータセットの最初の5個について感情分析をしてみます．ポジティブとネガティブの判定はどれも 0.5 くらいであり，判別しきれていません．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au8VIOje7Ct6"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ86BEs2iyKz"
      },
      "source": [
        "データセットを TensorFlow で処理できるように変換します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xQId7M0iy1x"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=True,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=False,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYxHbDIjYr9"
      },
      "source": [
        "学習の条件を設定し，学習を行います．ファインチューニング済みモデルを用いてテストデータセットの最初の5個の予測をしていますが，どれも判別の確率が上がっています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0l0ds5pEXNd"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=True,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=False,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    batch_size = 16\n",
        "    num_epochs = 5\n",
        "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "    optimizer, schedule = create_optimizer(\n",
        "        init_lr=2e-5, \n",
        "        num_warmup_steps=0, \n",
        "        num_train_steps=total_train_steps\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizer)\n",
        "    model.fit(\n",
        "        tf_train_dataset,\n",
        "        validation_data=tf_validation_dataset,\n",
        "        epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY0xwvGBkVad"
      },
      "source": [
        "ファインチューニング済みのモデルやトークナイザーは以下のように保存します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9oz2MDHkCcq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=True,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
        "        columns=['attention_mask', 'input_ids', 'label'],\n",
        "        shuffle=False,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    batch_size = 16\n",
        "    num_epochs = 5\n",
        "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "    optimizer, schedule = create_optimizer(\n",
        "        init_lr=2e-5, \n",
        "        num_warmup_steps=0, \n",
        "        num_train_steps=total_train_steps\n",
        "    )\n",
        "    \n",
        "    model.compile(optimizer=optimizer)\n",
        "    model.fit(\n",
        "        tf_train_dataset,\n",
        "        validation_data=tf_validation_dataset,\n",
        "        epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
        "\n",
        "    save_directory = './pretrained'\n",
        "    tokenizer.save_pretrained(save_directory)\n",
        "    model.save_pretrained(save_directory)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rfC8Kj03omW"
      },
      "source": [
        "```{note}\n",
        "終わりです．\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
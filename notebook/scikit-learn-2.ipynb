{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_xhvUO10XD6"
      },
      "source": [
        "# 教師なし学習法（編集中）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZQz79CS4WfU"
      },
      "source": [
        "## 教師なし学習法の種類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h6iDCxGOk2p"
      },
      "source": [
        "教師なし学習法には様々な種類があります．https://scikit-learn.org/stable/unsupervised_learning.html にはそれらがまとめられています．リンク先のページには以下に示すものが列挙されています．\n",
        "\n",
        "*   混合ガウスモデル\n",
        "*   多様体学習\n",
        "*   クラスタリング\n",
        "*   バイクラスタリング\n",
        "*   行列分解\n",
        "*   共分散推定\n",
        "*   外れ値検知\n",
        "*   密度推定\n",
        "*   制約付きボルツマンマシン\n",
        "\n",
        "中でも代表的なものには，階層的クラスタリング法，非階層的クラスタリング法（特に K-means），主成分分析法，t-SNE，カーネル密度推定法，自己組織化マップ，敵対的生成ネットワークがあります．教師なし学習法は主に与えられたデータの性質を理解するために利用されます．与えられたデータの中で類似しているインスタンスを集めるとか，与えられたデータの関係性を人間が理解しやすい方法（次元削減）で可視化するとかです．また，敵対的生成ネットワークはこれらとは異なり特殊な機械学習アルゴリズムで，新たなデータを生成するために利用されます．このコンテンツでは教師なし学習法の中でも scikit-learn を利用して簡単に実装できる最も代表的な手法の使い方を紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "敵対的生成ネットワークは別に紹介します．\n",
        "```"
      ],
      "metadata": {
        "id": "3mAPiZjtfkWx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcnoJlnfOk2r"
      },
      "source": [
        "次の節では scikit-learn を利用して，階層的クラスタリング法，K-means 法（非階層的クラスタリング法の代表的な手法），主成分分析法，カーネル密度推定法を実装します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JIH6ZHaOk2s"
      },
      "source": [
        "## 階層的クラスタリング法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRbtig5lOk2s"
      },
      "source": [
        "与えられたデータをクラスター化する階層的クラスタリング法の利用方法を紹介します．階層的クラスタリング法でクラスター化した各クラスターは階層構造を有します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQO6ztCj5CfH"
      },
      "source": [
        "### 基本的な事柄"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G70IuVgx5CfH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdFQn5noOk2s"
      },
      "source": [
        "## K-means 法"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "階層的クラスタリングと異なり K-means 法は非階層的にデータをクラスタ化する手法です．"
      ],
      "metadata": {
        "id": "Fl02i3UX32kh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRWiZH4r5ArL"
      },
      "source": [
        "### 基本的な事柄"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means 法はあらかじめ決めた $k$ 個のクラスタにデータを分割する方法です．K-means 法は以下のような手順で計算します．\n",
        "\n",
        "1.   ランダムに $k$ 個のクラスタの中心（$\\mu_k$）を決定します．\n",
        "2.   各インスタンスからそれぞれのクラスタの中心との距離を計算します．\n",
        "3.   各インスタンスを最も近いクラスタ中心のクラスタ（$C_k$）に所属させます．$C_k$ に所属するインスタンスの数を $n_k$ とします．\n",
        "4.   各クラスタの重心を計算して，その重心を新たなクラスタの中心とします．\n",
        "5.   クラスタの中心が変化しなくなるまで上の2, 3, 4の操作を繰り返します．\n",
        "\n",
        "よって，$\\mu_k$ は各インスタンスベクトルを $x_i$ としたとき，以下のように計算します．\n",
        "\n",
        "$\n",
        "\\displaystyle \\mu_k=\\frac{1}{n_k}\\sum_{i=1}^{n_k}x_i\n",
        "$\n",
        "\n",
        "各クラスタ内に所属するインスタンスとクラスタ中心との二乗距離の合計 $I_k$ は以下のように計算できますが，これをクラスタ内平方和とか慣性とかと呼びます．\n",
        "\n",
        "$\n",
        "\\displaystyle I_k=\\sum_{i=1}^{n_k}\\|x_i-\\mu_k\\|_2^2\n",
        "$\n",
        "\n",
        "K-means ではこの値を最初化するようにクラスタへのインスタンスの割り当てを行います．つまり，K-means では以下の $E$ を最小化します．\n",
        "\n",
        "$\\displaystyle E=\\sum_{i=1}^kI_k$"
      ],
      "metadata": {
        "id": "y7KQOx3y5ArQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クラスタリングの実行"
      ],
      "metadata": {
        "id": "wg_Md4BYs1_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは K-means 法を実行しますが，そのためのデータセットを生成します．scikit-learn にはクラスタリング法用に擬似的なデータセットを生成するためのユーティリティが備わっています．以下のようにすることで，3個のクラスタに分けられるべき150個のインスタンスを生成することができます．"
      ],
      "metadata": {
        "id": "Wzp-QsGns-Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "np.random.seed(1000)\n",
        " \n",
        "def main():\n",
        "    x, t = make_blobs(n_samples=150, centers=3)\n",
        "    plt.scatter(x[:,0], x[:,1])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZO8RG7433_RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "目で見ると3個のクラスタに分かれているように見えますね．\n",
        "```"
      ],
      "metadata": {
        "id": "-F1rKPWmGOp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このデータを最初に2個のクラスタに分けます．以下のようにします．"
      ],
      "metadata": {
        "id": "WBa25P4_GVnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "np.random.seed(1000)\n",
        " \n",
        "def main():\n",
        "    x, t = make_blobs(n_samples=150, centers=3)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2).fit(x)\n",
        "    cluster = kmeans.labels_\n",
        "\n",
        "    colors = [\"navy\", \"turquoise\"]\n",
        "    plt.figure()\n",
        "    for color, i in zip(colors, [0, 1]):\n",
        "        plt.scatter(x[cluster==i, 0], x[cluster==i, 1], color=color, alpha=0.8, lw=0, label=str(i))\n",
        "    plt.legend()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FZ1jmszMBJAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "微妙な感じがします．\n",
        "```"
      ],
      "metadata": {
        "id": "r0YvGinxGjAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に3個のクラスタに分割します．以下のように書きます．"
      ],
      "metadata": {
        "id": "G7ObwCvAGma_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "np.random.seed(1000)\n",
        " \n",
        "def main():\n",
        "    x, t = make_blobs(n_samples=150, centers=3)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=3).fit(x)\n",
        "    cluster = kmeans.labels_\n",
        "\n",
        "    colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
        "    plt.figure()\n",
        "    for color, i in zip(colors, [0, 1, 2]):\n",
        "        plt.scatter(x[cluster==i, 0], x[cluster==i, 1], color=color, alpha=0.8, lw=0, label=str(i))\n",
        "    plt.legend()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XGwE0bZMCsOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "これだって感じがしますね．\n",
        "```"
      ],
      "metadata": {
        "id": "1eS7giayGrYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "乱数の種は ` make_blobs(n_samples=150, centers=3, random_state=1000) ` のように指定することもできますが，ここで発生する乱数は NumPy の乱数発生機能に依存しているので NumPy を読み込んで ` np.random.seed() ` で行っても良いのです．\n",
        "```"
      ],
      "metadata": {
        "id": "54oczcQLG4L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クラスタ数の決定方法"
      ],
      "metadata": {
        "id": "BxpWKqQis-bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の例だと正解のクラスタ数を知っているので $k$ を上手に設定することができました．実際のデータをクラスタリングしたいときには何個に分割すれば良いか分からない場合が多いと思います．そのときに，$k$ の値を決定するための指標があります．シルエットスコアと言います．"
      ],
      "metadata": {
        "id": "ZdNdHIXWtD7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "シルエットスコアはクラスタに所属するインスタンス間の平均距離（凝集度と言う）をそのクラスタから最も近いクラスタに存在しているインスタンスとの平均距離（乖離度と言う）から引いた値を凝集度または乖離度の内で大きな値で割った値を全 $k$ 個のクラスタについて計算して平均をとった値です．\n",
        "```"
      ],
      "metadata": {
        "id": "xGoR3vqWIekK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "シルエットスコアは以下のように計算します．上と同じデータに対して，$k$ の値を変えて K-means 法を実行した場合のシルエットスコアを出力します．"
      ],
      "metadata": {
        "id": "BlNsbWmFIjOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "np.random.seed(1000)\n",
        " \n",
        "def main():\n",
        "    x, t = make_blobs(n_samples=150, centers=3)\n",
        "\n",
        "    for k in range(2,8):\n",
        "        kmeans = KMeans(n_clusters=k).fit(x)\n",
        "        cluster = kmeans.labels_\n",
        "        print(k, \": \", silhouette_score(x, cluster), sep=\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "86qrWF9EIiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "シルエットスコアはクラスタの個数が3個のとき最も良い値を示しており，やはり，クラスタ数は3個が良さそうであることが確認できます．"
      ],
      "metadata": {
        "id": "l0NeKZYwJSR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{attention}\n",
        "クラスタ数を決定付ける強力な根拠ではありません．こういう指標があるという程度に使ってください．\n",
        "```"
      ],
      "metadata": {
        "id": "401XenKDJc0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{hint}\n",
        "他にはクラスタ数に対するクラスタ内平方和の変化を観察するエルボー法という方法があります．\n",
        "```"
      ],
      "metadata": {
        "id": "34NfOdZ4JsHo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwUb15zOk2t"
      },
      "source": [
        "## 主成分分析法"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "主成分分析法の進め方を紹介します．"
      ],
      "metadata": {
        "id": "g9buCNdA3x4d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFRSh4DUOk2t"
      },
      "source": [
        "### 基本的な事柄"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "与えられたデータに対して主成分分析を行う理由は，人が目で見て理解するには複雑なデータを人が理解しやすい形式に整えたいためです．4個以上の要素からなるベクトル形式のデータを2個または3個（または稀に1個）の要素からなるベクトル形式のデータに変換し2次元または3次元（または1次元）平面上にインスタンスをプロットし直すことでデータの関連性を把握することができます．"
      ],
      "metadata": {
        "id": "p_a-bipAE7Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "この変換を次元削減と言います．\n",
        "```"
      ],
      "metadata": {
        "id": "P8jM9DPiFjsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "主成分分析で行っていることはちょうど以下のような作業です．左に示されている2次元平面上にプロットされた点を右に示されているように1次元平面上にプロットしています．\n",
        "\n",
        "<img src=\"../image/pca_01.svg\" width=\"70%\" />"
      ],
      "metadata": {
        "id": "YJsDeebXFpc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ただし，主成分分析で行いたいことは以下のような次元削減ではありません．以下のようにすると変換後の軸上のインスタンスが互いに重なっており，かなりの情報が失われているように思えます．\n",
        "\n",
        "<img src=\"../image/pca_02.svg\" width=\"70%\" />"
      ],
      "metadata": {
        "id": "So5QvSDt3iAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "主成分分析では元の情報をできるだけ維持したままデータの変換をしようとします．この例において新たに生成される軸はこのデータを説明するための情報量が最も大きい方向に設定されます．情報量が最も大きい方向とはデータが最も散らばっている（分散が大きい）方向です．\n",
        "```"
      ],
      "metadata": {
        "id": "wmkSh6SjGWA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 主成分分析法の限界"
      ],
      "metadata": {
        "id": "iBdk4s8yJRwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "主成分分析は変数を元の変数の線形結合で表される新たな変数へと変換させる方法です．元々何らかの非線形平面で関係を持っていたデータを別の平面へと変換した場合において，元々の非線形な関係性が維持されているとは限りません．非線形な関係を含めて次元削減をしたい場合は他の方法を利用する方法があります．主成分分析法を非線形で行う方法には非線形カーネル関数を利用したカーネル主成分分析法があります．scikit-learn でも利用することが可能です．"
      ],
      "metadata": {
        "id": "Fd7RF524JjlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 次元の削減"
      ],
      "metadata": {
        "id": "kydMpbOvL6Cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "アヤメのデータセットに対して次元の削減を行います．以下のようにこのデータセットにおける各インスタンスは4個の要素からなるベクトル形式のデータです．よってこれを4次元平面上にプロットしたとしてもその関係性を人は理解できません．"
      ],
      "metadata": {
        "id": "pxFC4lEtONP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "そもそも4次元平面なんて描画できませんね．\n",
        "```"
      ],
      "metadata": {
        "id": "A-ump4SQRq22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        " \n",
        "def main():\n",
        "    diris = load_iris()\n",
        "    print(diris.data[0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ILDwt1JBOaJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "主成分分析は以下のように行います．主成分分析で得られた全インスタンス（150個）の値を出力させます．"
      ],
      "metadata": {
        "id": "GCB6drj-RvHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        " \n",
        "def main():\n",
        "    diris = load_iris()\n",
        "    x = diris.data\n",
        "    t = diris.target\n",
        "    target_names = diris.target_names\n",
        "    pca = PCA(n_components=2) # n_componentsで縮約後の次元数を指定します．\n",
        "    xt = pca.fit(x).transform(x)\n",
        "    print(xt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dxIpU697PE36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次元縮約後の各インスタンスを以下のコードで散布図上にプロットします．"
      ],
      "metadata": {
        "id": "jniLBBnCSE5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        " \n",
        "def main():\n",
        "    diris = load_iris()\n",
        "    x = diris.data\n",
        "    t = diris.target\n",
        "    target_names = diris.target_names\n",
        "    pca = PCA(n_components=2)\n",
        "    xt = pca.fit(x).transform(x)\n",
        "    \n",
        "    plt.figure()\n",
        "    colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
        "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "        plt.scatter(xt[t==i, 0], xt[t==i, 1], color=color, alpha=0.8, lw=0, label=target_name)\n",
        "    plt.legend()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QTbgoT-1PwTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各種類のアヤメがそれぞれ集まっていることがわかります．2次元平面上にプロットすることで各インスタンスの関係性を把握することができました．この主成分平面には横軸と縦軸がありますが，これらの軸が何を意味しているのかは解析者がデータの分布の様子を観察する等して決定しなければなりません．"
      ],
      "metadata": {
        "id": "NNm6BAoBSN5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "軸の意味の解釈のヒントは主成分負荷量を散布図上にプロットすることである程度は得られます．主成分負荷量とは最終的に得られた固有ベクトル（線形結合の係数）にそれに対応する固有値の正の平方根を掛けた値のことです．\n",
        "```"
      ],
      "metadata": {
        "id": "5_kjgOZukhIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "この主成分平面上の任意の点をサンプリングして主成分分析の逆操作をすると新たなデータを生成することも可能です．\n",
        "```"
      ],
      "metadata": {
        "id": "4s9SAwAeS0e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        " \n",
        "def main():\n",
        "    diris = load_iris()\n",
        "    x = diris.data\n",
        "    t = diris.target\n",
        "    target_names = diris.target_names\n",
        "    pca = PCA(n_components=2)\n",
        "    xt = pca.fit(x).transform(x)\n",
        "    \n",
        "    plt.figure()\n",
        "    colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
        "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "        plt.scatter(xt[t==i, 0], xt[t==i, 1], color=color, alpha=0.8, lw=0, label=target_name)\n",
        "    plt.legend()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lkcy96V1dtFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 次元削減データの説明力"
      ],
      "metadata": {
        "id": "OHCt-1HeTAll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これまでの計算で4つの要素からなるベクトルデータを2つの要素からなるベクトルデータへと変換しました．そのインスタンスを特徴付ける4つの要素を半分にしたのですから元々インスタンスが持っていた情報は少なくなっているはずです．この主成分分析の操作でどれくらいの情報が失われたのか，どれくらいの情報が維持されているのかは以下のコードで確認できます．元々の情報を1としたときに各軸が持つ説明力の割合を出力することができます．また，それらの値を合計することで元々の情報を2つの軸だけでどれくらい説明できるかを計算できます．"
      ],
      "metadata": {
        "id": "-6mQlB7he2Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        " \n",
        "def main():\n",
        "    diris = load_iris()\n",
        "    x = diris.data\n",
        "    t = diris.target\n",
        "    target_names = diris.target_names\n",
        "    pca = PCA(n_components=2)\n",
        "    xt = pca.fit(x).transform(x)\n",
        "    print(pca.explained_variance_ratio_) # 各軸が持つ説明力の割合．\n",
        "    print(sum(pca.explained_variance_ratio_)) # 2軸で説明できる割合．\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Tz-w8pVjeM7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "第一主成分のみで全体の大体92%の説明力を持ち，第二主成分で大体5%の説明力を持つようです．ふたつの軸によって元の約98%の説明ができているようです．"
      ],
      "metadata": {
        "id": "HkPUVj9HgA_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "この説明力の比率は寄与率と言います．それらを（解析者が必要と感じる次元数まで）足したものを累積寄与率と言います．\n",
        "```"
      ],
      "metadata": {
        "id": "6B88cryYgch9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxolXhqEOk2t"
      },
      "source": [
        "## カーネル密度推定法"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "カーネル密度推定法の利用方法を紹介します．簡単な利用方法に加えて生成モデルとして利用する方法を紹介します．"
      ],
      "metadata": {
        "id": "CWArKLLI3nPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 基本的な事柄"
      ],
      "metadata": {
        "id": "jZTDZ9lperCn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Mh-KoTOk2t"
      },
      "source": [
        "カーネル密度推定法は与えられたデータの分布を推定する方法です．与えられたデータ中の疎なインスタンスを入力としてそのデータが従うと思われる分布を推定する方法です．$x_1, x_2, \\dots, x_n$ を何らかの確率分布から得られたサンプルとします．このときにカーネル密度推定量 $f$ は以下のように計算されます．\n",
        "\n",
        "$\n",
        "\\displaystyle f(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\left(\\frac{x-x_i}{h}\\right)\n",
        "$\n",
        "\n",
        "このとき，$K$ はカーネル関数と呼ばれる確率分布を近似するための関数で，$h$ はバンド幅と呼ばれるハイパーパラメータです．カーネル関数として利用される関数には様々なものがありますが，以下の標準正規分布（平均値が0で分散が1である正規分布）の確率密度関数を利用することが多いです．\n",
        "\n",
        "$\n",
        "\\displaystyle K(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "このカーネルのことはガウシアンカーネルとも言いますね．\n",
        "```"
      ],
      "metadata": {
        "id": "6ShGJD1icm9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 発展的な利用方法"
      ],
      "metadata": {
        "id": "EqZ5v9xgeuH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "昨今の深層学習ブームで専ら利用されている深層ニューラルネットワークは通常たくさんのハイパーパラメータを持ちます．良いハイパーパラメータを同定することは良い人工知能を構築するために重要なことであるため，その探索方法の開発が活発です．ブルートフォース（しらみつぶし）な探索，ランダムな探索，手動による探索，進化戦略法を利用した探索，ベイズ探索等の様々な方法が利用されていますが，その中でも最も有効なもののひとつに代理モデルを利用した逐次最適化法（sequential model-based optimization（SMBO））と呼ばれるベイズ最適化法の範疇に入る方法があります．ハイパーパラメータが従うであろう分布を推定して（この推定した分布を代理モデルと言います），その分布から予想したハイパーパラメータを利用して構築した人工知能の評価を行い，さらにその評価結果から分布の推定を繰り返す，というようなハイパーパラメータの従う分布の推定と人工知能の評価を交互に繰り返すことで最適なハイパーパラメータを持つ人工知能を同定しようとする方法です．この SMBO を行う際の代理モデルの構築にカーネル密度推定法が利用されることがあります．そして，カーネル密度推定法を利用した SMBO は従来の代理モデルの推定法（例えば，ガウス過程回帰法）より良い性能を示すことがあります．"
      ],
      "metadata": {
        "id": "TXpe9scXfcjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "SMBO の領域ではカーネル密度推定量はパルツェン推定量と呼ばれています．\n",
        "```"
      ],
      "metadata": {
        "id": "RnrjFTUaiDku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "ハイパーパラメータの最適化は深層学習の分野で最も重要なトピックのひとつなので紹介してみました．\n",
        "```"
      ],
      "metadata": {
        "id": "uVkVj79MiSMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 元の確率分布の推定"
      ],
      "metadata": {
        "id": "ZA7TKdR4e6gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは，母数の異なるふたつの正規分布からいくつかのインスタンスをサンプリングして，そのサンプリングしたデータから元の正規分布ふたつからなる二峰性の確率分布を再現できるかということを試します．"
      ],
      "metadata": {
        "id": "BlaLyet4fB4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{hint}\n",
        "正規分布の母数（パラメータ）は平均値と分散ですね．母数の値が決まればそれに対応する正規分布の形状は一意に決まるのですね．\n",
        "```"
      ],
      "metadata": {
        "id": "1txwtm2K1oX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードで $N(-3, 1.5)$ と $N(-3, 2)$ の正規分布を描画します．"
      ],
      "metadata": {
        "id": "ERhAr5LjSau5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        " \n",
        "def main():\n",
        "    x = np.linspace(-8, 8, 100)\n",
        "    y = (norm.pdf(x, loc=2, scale=1.5) + norm.pdf(x, loc=-3, scale=2)) / 2\n",
        "    plt.plot(x, y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Tqrlx5Qj2ZGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，以下のコードで $N(-3, 1.5)$ に従う50個のインスタンスと $N(-3, 2)$ に従う50個のインスタンスをサンプリングします．また，そのヒストグラムを描きます．"
      ],
      "metadata": {
        "id": "11x87fk-S6r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "np.random.seed(0)\n",
        " \n",
        "def main():\n",
        "    x1 = norm.rvs(loc=2, scale=1.5, size=50)\n",
        "    plt.hist(x1)\n",
        "    x2 = norm.rvs(loc=-3, scale=2, size=50)\n",
        "    plt.hist(x2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "k3ejvHBDTFeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{attention}\n",
        "計算機実験をする際は乱数の種は固定しなきゃならないのでしたね．\n",
        "```"
      ],
      "metadata": {
        "id": "ZP57wd1MTtrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "カーネル密度推定は以下のように行います．"
      ],
      "metadata": {
        "id": "ueNQ6kYSb36Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)\n",
        " \n",
        "def main():\n",
        "    x1 = norm.rvs(loc=2, scale=1.5, size=1000)\n",
        "    x2 = norm.rvs(loc=-3, scale=2, size=1000)\n",
        "    x = np.concatenate([x1, x2]) # x1とx2を連結します\n",
        "    x = x.reshape(-1, 1) # このような入力形式にしないと受け付けてくれないからこうしました．\n",
        "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.4).fit(x) # ハンド幅は適当に選んでみました．\n",
        "    p = np.linspace(-8, 8, 100)[:, np.newaxis] # プロット用の値を生成しています．\n",
        "    l = kde.score_samples(p) # これで予測値を計算します．\n",
        "    plt.plot(p, np.exp(l)) # 予測値は対数値で出力されているのでそれをnp.exp()を利用してプロットします．\n",
        "    y = (norm.pdf(p, loc=2, scale=1.5) + norm.pdf(p, loc=-3, scale=2)) / 2 # 元の分布です．\n",
        "    plt.plot(p, y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-JHa8o_0Ul3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "サンプリングしたインスタンスを使って予測した分布の形状と元の分布の形状が類似している様子がわかります．\n",
        "```"
      ],
      "metadata": {
        "id": "U92DvL3cb7PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 生成モデルとしての利用"
      ],
      "metadata": {
        "id": "A6pr7p42fCTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "カーネル密度推定法は何もないところからデータを生成する生成モデルとして利用することができます．何らかのデータを入力にしてそのデータが出力される確率分布をカーネル密度推定法で推定します．次に，その確率密度分布に基づいてデータを生成する，といった手順です．"
      ],
      "metadata": {
        "id": "CCJO2c0YyDqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{hint}\n",
        "与えられたデータが出力された確率分布を推定できたのなら，その分布から新たなデータは当然出力することができるよねという仕組みです．\n",
        "```"
      ],
      "metadata": {
        "id": "dJKAB4-JzgOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは機械学習界隈で最も有名なデータセットである MNIST（Mixed National Institute of Standards and Technology database）を解析対象に用います．「エムニスト」と発音します．MNIST は縦横28ピクセル，合計784ピクセルよりなる画像データです．画像には手書きの一桁の数字（0から9）が含まれています．公式ウェブサイトでは，学習データセット6万個とテストデータセット1万個，全部で7万個の画像からなるデータセットが無償で提供されています．そのデータセットを以下のようにダウンロードして最初のデータを可視化します．"
      ],
      "metadata": {
        "id": "iLTieOzzzuhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "    (x, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    plt.imshow(x[0], cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()"
      ],
      "metadata": {
        "id": "mGVR2UNLy23i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{note}\n",
        "これは5ですね．\n",
        "```"
      ],
      "metadata": {
        "id": "rSZs14Nfz2Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のようにすることで新たな画像データを生成することができます．学習済みの生成器に対して利用する `.sample()` というメソッドで新たなデータを生成することができます．"
      ],
      "metadata": {
        "id": "bysDx9iiz8Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "np.random.seed(0)\n",
        "\n",
        "def main():\n",
        "    (x, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    x = x.reshape(-1, 28*28) # 縦横どちらも28ピクセルの画像を784の要素からなるベクトルに変換します．\n",
        "    kde = KernelDensity().fit(x)\n",
        "    g = kde.sample(4) # 学習済みの生成器で4個の画像を生成させてみます．\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(len(g)): # 生成データの可視化です．\n",
        "        s = g[i].reshape(28, 28)\n",
        "        plt.subplot(1, 4, i+1)\n",
        "        plt.imshow(s, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zu4ZCq2xRfab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "左から，「2」，「1」，「1」，「6」という画像が生成されているように見えます．"
      ],
      "metadata": {
        "id": "CJENHmNN0nbm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rfC8Kj03omW"
      },
      "source": [
        "```{note}\n",
        "終わりです．\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "scikit-learn-2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
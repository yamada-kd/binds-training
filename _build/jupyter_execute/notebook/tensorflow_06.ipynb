{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_xhvUO10XD6"
   },
   "source": [
    "# アテンションネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLJitgHzn8VQ"
   },
   "source": [
    "```{note}\n",
    "作成中．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Sx06lGrg3K"
   },
   "source": [
    "## アテンションとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0A5sjHenmnI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EliP-z_krg3N"
   },
   "source": [
    "### トランスフォーマー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3P_sTkDzu4O"
   },
   "source": [
    "トランスフォーマーとはアテンションとポジショナルエンコードといわれる技術を用いて，再帰型ニューラルネットワークとは異なる方法で文字列を処理することができるニューラルネットワークの構造です．機械翻訳や質問応答に利用することができます．\n",
    "\n",
    "例えば，機械翻訳の場合，翻訳したい文字列を入力データ，翻訳結果の文字列を教師データとして利用します．構築した人工知能は翻訳したい文字列を入力値として受け取り，配列を出力します．配列の各要素は文字の個数と同じサイズのベクトル（その要素が何の文字なのかを示す確率ベクトル）です．\n",
    "\n",
    "トランスフォーマーはエンコーダーとデコーダーという構造からなります．エンコーダーは配列（機械翻訳の場合，翻訳したい配列）を入力にして，同じ長さの配列を出力します．デコーダーも配列（機械翻訳の場合，翻訳で得たい配列）とエンコーダーが出力した配列を入力にして同じ長さの配列（各要素は確率ベクトル）を出力します．エンコーダーが出力した配列情報をデコーダーで処理する際にアテンションという技術が利用されます．\n",
    "\n",
    "<img src=\"https://github.com/yamada-kd/binds-training/blob/main/image/transformer.svg?raw=1\" width=\"100%\" />\n",
    "\n",
    "エンコーダーとデコーダー間のアテンション以外にも，エンコーダーとデコーダーの内部でもそれぞれアテンション（セルフアテンション）が計算されます．アテンションは文字列内における文字の関連性を計算します．\n",
    "\n",
    "トランスフォーマーは再帰型ニューラルネットワークで行うような文字の逐次的な処理が不要です．よって，計算機の並列化性能をより引き出せます．扱える文脈の長さも無限です（再帰型ニューラルネットワークでも理論上無限です．）．\n",
    "\n",
    "このトランスフォーマーはものすごい性能を発揮しており，これまでに作られてきた様々な構造を過去のものとしました．特に応用の範囲が広いのはトランスフォーマーのエンコーダーの部分です．BERT と呼ばれる方法を筆頭に自然言語からなる配列を入力にして何らかの分散表現を出力する方法として自然言語処理に関わる様々な研究開発に利用されています．\n",
    "\n",
    "（会話でトランスフォーマーという場合は，トランスフォーマーのエンコーダーまたはデコーダーのことを言っている場合があります．エンコーダー・デコーダー，エンコーダー，デコーダー，この3個でそれぞれできることが異なります．）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8ViTfSl3gha"
   },
   "source": [
    "```{hint}\n",
    "実用上，配列を入力にして配列を返す構造とだけ覚えておけば問題はないと思います．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isuh1_iiygT0"
   },
   "source": [
    "### できること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtIMcWPodXRj"
   },
   "source": [
    "Hugging Face で扱うことができるタスクは以下に示すものがあります．これ以外にもありますが自然言語処理に関する代表的なタスクを抽出しました．括弧内の文字列は実際に Hugging Face を利用する際に指定するオプションです（後で利用します．）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRGjF8pHdtO1"
   },
   "source": [
    "    \n",
    "\n",
    "*   感情分析（`sentiment-analysis`）：入力した文章が有する感情を予測\n",
    "*   特徴抽出（`feature-extraction`）：入力した文章をその特徴を示すベクトルに変換\n",
    "*   穴埋め（`fill-mask`）：文章中のマスクされた単語を予測\n",
    "*   固有表現抽出（`ner`）：入力した文章中の固有表現（名前とか場所とか）にラベルをつける\n",
    "*   質問応答（`question-answering`）：質問文とその答えが含まれる何らかの説明文を入力として解答文を生成\n",
    "*   要約（`summarization`）：入力した文章を要約\n",
    "*   文章生成（`text-generation`）：文章を入力にして，その文章に続く文章を生成\n",
    "*   翻訳（`translation`）：文章を他の言語に翻訳\n",
    "*   ゼロショット文章分類（`zero-shot-classification`）：文章とそれが属する可能性があるいくつかのカテゴリを入力にしてその文章をひとつのカテゴリに分類\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3mzuNEjy5to"
   },
   "source": [
    "### Hugging Face Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7fpi958zLiS"
   },
   "source": [
    "Hugging Face の利用方法は以下のウェブサイト，Hugging Face Course の Transformer models の部分を読めば大体のことが把握できると思います．\n",
    "\n",
    "https://huggingface.co/course/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hezey89MynbX"
   },
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IG4DcIBO42U_"
   },
   "source": [
    "Hugging Face（のトランスフォーマー）は TensorFlow または PyTorch とあわせて利用可能なライブラリです．本来は TensorFlow か PyTorch をあらかじめインストールする必要があります．グーグルコラボラトリーには既に TensorFlow がインストールされているため不要です．以下のように `transformers` のみをインストールすれば利用可能です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp0yFo3Se8PB"
   },
   "outputs": [],
   "source": [
    "! pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYrJO84QpmSM"
   },
   "source": [
    "```{attention}\n",
    "このコンテンツの実行にグーグルコラボラトリーを使っておらず，自身の環境で Anaconda 等を利用している場合は気をつけてください．例えば，Anaconda を利用しているのであれば ` conda install -c huggingface transformers ` のようなコマンドでインストールした方が良いです．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RlLdfs5ynbW"
   },
   "source": [
    "## 基本的な使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZpHl3AlpmSN"
   },
   "source": [
    "とても簡単に自然言語処理を実現することができる利用方法を紹介します．世界最高性能を求めたいとかでないなら，ここで紹介する方法を利用して様々なことを達成できると思います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saOu-Onk5d-w"
   },
   "source": [
    "### 感情分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_RY-XjT5d-w"
   },
   "source": [
    "最も簡単な `tranformers` の利用方法は以下のようになると思います．`pipeline` を読み込んで，そこに取り組みたいタスク（`sentiment-analysis`）を指定します．初回の起動の際には事前学習済みモデルがダウンロードされるため時間がかかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4QgsbGyfBA3"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    text = \"I have a pen.\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4KbHIJHDFk"
   },
   "source": [
    "入力した文章がポジティブな文章なのかネガティブな文章なのかを分類できます．ここでは1個の文章を入力しましたが，以下のように2個以上の文章も入力可能です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVrVwjvFHVax"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    litext = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    "    result = classifier(litext)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEBdCL6KcIo8"
   },
   "source": [
    "### 特徴抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5N2rUuWcIo_"
   },
   "source": [
    "文字列の特徴を抽出して何らかの分散表現にしたい場合は `feature-extraction` を指定します．文字列の長さに依存した配列が出力されますが，同じ長さのベクトルにしたい場合は配列長に渡って要素の値を足し算すること等で特徴量を得ることができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbEy3hgncIpA"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    converter = pipeline(\"feature-extraction\")\n",
    "    text = \"We are very happy to introduce pipeline to the transformers repository.\"\n",
    "    result = converter(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIhMpetYd9SW"
   },
   "source": [
    "```{hint}\n",
    "例えば，BERT を使うと各トークンが768次元のベクトルでトークン数個からなる要素のベクトルが出力されます．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjztlAuqHzky"
   },
   "source": [
    "### ゼロショット文章分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M6_h785Hzky"
   },
   "source": [
    "ゼロショット文章分類は以下のように利用します．ゼロショットとは訓練中に一度も出現しなかったクラスの分類タスクです．ラベルが未定義の問題に利用することができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAXUlTWnIEVE"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"zero-shot-classification\")\n",
    "    text = \"This is a course about the Transformers library\",\n",
    "    lilabel = [\"education\", \"politics\", \"business\"]\n",
    "    result = classifier(text, candidate_labels = lilabel)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfsZyVRAIbZo"
   },
   "source": [
    "```{note}\n",
    "ゼロショット分類では，例えば，馬を入力にして猫とか犬とかのラベルそのものを予測されるのではなくて猫および犬ベクトルを予測させどちらに近いかを予測させることができます．そろそろマシンパワー的にきついかもしれませんね．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eW9CmBX5p2K"
   },
   "source": [
    "### 文章生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgHhNYYjiTvR"
   },
   "source": [
    "文章の生成は以下のように行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ean4ulzOIy54"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    generator = pipeline(\"text-generation\")\n",
    "    text = \"In this course, we will teach you how to\"\n",
    "    result = generator(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13EIb-OP5xoz"
   },
   "source": [
    "### 穴埋め"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWPChdEo5l_x"
   },
   "source": [
    "穴埋めは以下のように行います．以下のコードでは可能性が高いものふたつを表示させます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4MA5Uw6JYiw"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    unmasker = pipeline(\"fill-mask\")\n",
    "    text = \"This course will teach you all about <mask> models.\"\n",
    "    result = unmasker(text, top_k=2)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5morCbD95ytz"
   },
   "source": [
    "### 固有表現抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDj3vKYP5yt0"
   },
   "source": [
    "固有表現抽出は以下のように行います．結果の `PER` は人名，`ORG` は組織名，`LOC` は地名です．それらの固有表現の文書中における位置も `start` と `end` で示されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKnlUcasJeVW"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    text = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "    result = ner(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtflLBrq5zhl"
   },
   "source": [
    "### 質問応答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRqwDbBK5zhl"
   },
   "source": [
    "SQuAD のような機械学習コンテストで行われる質問応答は質問だけを入力にして何かを出力する問題ではありません．質問文と何らかの説明文を入力にして解答を出力される問題です．以下のように利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJdXGoVRJfG_"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    question_answerer = pipeline(\"question-answering\")\n",
    "    question_text = \"Where do I work?\"\n",
    "    explanation = \"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    "    result = question_answerer(question = question_text, context = explanation)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-GL03sOJj2z"
   },
   "source": [
    "### 要約"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GgSz_UKJj2z"
   },
   "source": [
    "文章の要約は以下のように行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BarS-2rN0u7"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    text = \"\"\"\n",
    "        America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "    \"\"\"\n",
    "    result = summarizer(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU_diMPDJ8Je"
   },
   "source": [
    "### 翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRwU4giyJ8Jf"
   },
   "source": [
    "翻訳はこれまでと少しだけ指定方法が異なります．以下のように `translation_XX_to_YY` としなければなりません．ここでは，英語からフランス語への翻訳を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Iv7oI1PJ8Jf"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    translator = pipeline(\"translation_en_to_fr\")\n",
    "    text = \"This course is produced by Hugging Face.\"\n",
    "    result = translator(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqwWY1c0O02R"
   },
   "source": [
    "## 応用的な使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pjsr5ACupmST"
   },
   "source": [
    "これまでに利用したものとは異なるモデルを利用したいとか，自身が持っているデータセットにより適合させたいとかの応用的な利用方法を紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSKzyoA1O02W"
   },
   "source": [
    "### 他のモデルの利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G316vauoido7"
   },
   "source": [
    "これまでに，Hugging Face が自動でダウンロードしてくれたデフォルトの事前学習済みモデルを利用した予測を行いましたが，そうでなくて，モデルを指定することもできます．以下のページをご覧ください．Model Hub と言います．\n",
    "\n",
    "https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "\n",
    "\n",
    "この Model Hub の Tasks というところでタグを選択できます．例えば，Text Generation の `distilgpt2` を利用するには以下のように書きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEduyYGdPhf0"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    text = \"In this course, we will teach you how to\"\n",
    "    result = generator(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z4xVexiVKfN"
   },
   "source": [
    "### 日本語の解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wK2cF2YVKfR"
   },
   "source": [
    "日本語も扱うことができます．ここでは，日本語で書かれた文章の感情分析を行います．最初に，必要なライブラリをダウンロードしてインストールします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qknFQF2XVe9e"
   },
   "outputs": [],
   "source": [
    "! pip install fugashi\n",
    "! pip install ipadic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-odvcOynXvTH"
   },
   "source": [
    "```{note}\n",
    "これをインストールしないで使ったらインストールしろってメッセージが出たからインストールしました．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4Zjx7FQpmSU"
   },
   "source": [
    "```{attention}\n",
    "グーグルコラボラトリーでのインストール方法です．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLSh2N3ZX6r7"
   },
   "source": [
    "Model Hub で調べたら以下のような感情分析のモデルが公開されていたので，それを使います．トークナイザーとは文章をトークン化（単語化して数字を割り当てます）してくれるものです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGPJoEbVVKfR"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    "\n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=\"daigo/bert-base-japanese-sentiment\", tokenizer=\"daigo/bert-base-japanese-sentiment\")\n",
    "    text = \"みんながマリオのチョコエッグツイートをしていく中，未だにひとつも買えなくて焦る…．\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A99SjtkzYGhL"
   },
   "source": [
    "以下のようにモデルやトークナイザーは明示的に書くこともできます．ここでは，東北大学の乾研が公開している日本語版BERTを利用してみました．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AicM55hXPPS"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "def main():\n",
    "    mytokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "    unmasker = pipeline(\"fill-mask\", model=\"cl-tohoku/bert-base-japanese-whole-word-masking\", tokenizer=mytokenizer)\n",
    "    text = \"みんなが[MASK]のチョコエッグツイートをしていく中，未だにひとつも買えなくて焦る…．\"\n",
    "    result = unmasker(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UNdPpS3qMjv"
   },
   "source": [
    "### モデルの設定変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt3UNnsZqMjz"
   },
   "source": [
    "上で紹介したのと同様に，モデルとトークナイザーをそれぞれ，`TFAutoModelForSequenceClassification` と `AutoTokenizer` で読み込むことができます．以下の通りです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riYXQjfbqMjz"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def main():\n",
    "    mymodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    mytokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
    "    text = \"I do not have a pen but I am happy.\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BnWxkMStaGv"
   },
   "source": [
    "この際に `Auto` を使わずにあらかじめ用意されていいるクラスを明示的に指定することもできます．この場合，以下のように書きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oyaw5dTte0a"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "def main():\n",
    "    mymodel = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    mytokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
    "    text = \"I do not have a pen but I am happy.\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_DAQNqrtnxe"
   },
   "source": [
    "さらにモデルをカスタマイズすることもできます．ここでは，`DistilBert` の構造を変えてしまっているので，全く性能が出ていません．学習をし直す必要がありそうです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niN8Oniptu20"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "def main():\n",
    "    myconfig = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
    "    mymodel = TFDistilBertForSequenceClassification(myconfig)\n",
    "    mytokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=mymodel, tokenizer=mytokenizer)\n",
    "    text = \"I do not have a pen but I am happy.\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQwtexEFY0jp"
   },
   "source": [
    "### ファインチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i__HcsHYY0ju"
   },
   "source": [
    "事前学習モデルを自分の解きたい問題に合わせてファインチューニングすることができます．ここでは，インターネット上の映画レビューのデータセット IMDb に対してモデルのファインチューニングを行います．最初に Hugging Face が提供してくれているデータセットを利用するためのモジュールをインストールします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9CAu2TXwo5a"
   },
   "outputs": [],
   "source": [
    "! pip3 install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spMlnpdgwyzM"
   },
   "source": [
    "中身を確認します．映画に関するレビューが含まれています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5DEA0mrwxdl"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    print(imdb[\"train\"][0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak6bsiQbxTjd"
   },
   "source": [
    "このデータセットをトークナイズします．以下のようなコードを追加します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyITun0txTjd"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    print(tokenized_imdb)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMbsATli53L7"
   },
   "source": [
    "データセット中のデータをパディングします．データセット中の最大の長さのデータに合わせてパディングしても良いのですが，それだと非効率的なので，バッチ毎にパディングする方法を行います．ダイナミックパディングと呼ばれる方法です．以下のようにします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKnzh27_6P6u"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2dQT4Zv8iEC"
   },
   "source": [
    "ファインチューニング前のモデルを呼び出して，テストデータセットの最初の5個について感情分析をしてみます．ポジティブとネガティブの判定はどれも 0.5 くらいであり，判別しきれていません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au8VIOje7Ct6"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ86BEs2iyKz"
   },
   "source": [
    "データセットを TensorFlow で処理できるように変換します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xQId7M0iy1x"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
    "\n",
    "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "        columns=['attention_mask', 'input_ids', 'label'],\n",
    "        shuffle=True,\n",
    "        batch_size=16,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "        columns=['attention_mask', 'input_ids', 'label'],\n",
    "        shuffle=False,\n",
    "        batch_size=16,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsYxHbDIjYr9"
   },
   "source": [
    "学習の条件を設定し，学習を行います．ファインチューニング済みモデルを用いてテストデータセットの最初の5個の予測をしていますが，どれも判別の確率が上がっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0l0ds5pEXNd"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
    "\n",
    "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "        columns=['attention_mask', 'input_ids', 'label'],\n",
    "        shuffle=True,\n",
    "        batch_size=16,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "        columns=['attention_mask', 'input_ids', 'label'],\n",
    "        shuffle=False,\n",
    "        batch_size=16,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    batch_size = 16\n",
    "    num_epochs = 5\n",
    "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
    "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "    optimizer, schedule = create_optimizer(\n",
    "        init_lr=2e-5, \n",
    "        num_warmup_steps=0, \n",
    "        num_train_steps=total_train_steps\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.fit(\n",
    "        tf_train_dataset,\n",
    "        validation_data=tf_validation_dataset,\n",
    "        epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY0xwvGBkVad"
   },
   "source": [
    "ファインチューニング済みのモデルやトークナイザーは以下のように保存します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9oz2MDHkCcq"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, create_optimizer, TFAutoModelForSequenceClassification, pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
    "\n",
    "    tf_train_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "        columns=['attention_mask', 'input_ids', 'label'],\n",
    "        shuffle=True,\n",
    "        batch_size=16,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    tf_validation_dataset = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "        columns=['attention_mask', 'input_ids', 'label'],\n",
    "        shuffle=False,\n",
    "        batch_size=16,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    batch_size = 16\n",
    "    num_epochs = 5\n",
    "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
    "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "    optimizer, schedule = create_optimizer(\n",
    "        init_lr=2e-5, \n",
    "        num_warmup_steps=0, \n",
    "        num_train_steps=total_train_steps\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.fit(\n",
    "        tf_train_dataset,\n",
    "        validation_data=tf_validation_dataset,\n",
    "        epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    print(classifier(imdb[\"test\"][0:5][\"text\"]))\n",
    "\n",
    "    save_directory = './pretrained'\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    model.save_pretrained(save_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfC8Kj03omW"
   },
   "source": [
    "```{note}\n",
    "終わりです．\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_06.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
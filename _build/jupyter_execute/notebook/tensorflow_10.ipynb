{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_xhvUO10XD6"
   },
   "source": [
    "# 強化学習法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxxrI-R1zUlU"
   },
   "source": [
    "```{note}\n",
    "作成途中\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Sx06lGrg3K"
   },
   "source": [
    "## 基本的な事柄\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-S6ghDZpmSG"
   },
   "source": [
    "この章では強化学習法を紹介します．中でも Q 学習と呼ばれる方法を紹介します．その後，深層学習法を利用して Q 学習を実行する深層 Q 学習の紹介をします．この節ではそれらを理解するために必要な基礎知識の説明をします．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EliP-z_krg3N"
   },
   "source": [
    "### 強化学習法とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3P_sTkDzu4O"
   },
   "source": [
    "機械学習法は大きく分けて，教師あり学習法，教師なし学習法，強化学習法の 3 個に分類されます．教師あり学習法は入力データとそれに紐づいた教師データからなるデータを利用して，入力データに対して教師データに応じた出力をするように人工知能（機械学習モデル）を成長させるものです．強化学習法で成長させるものはエージェントと呼ばれる人工知能です．強化学習法をする際の登場人物は環境とエージェントのふたつです．エージェントが何らかの根拠に基づいて環境に作用します．例えば，ゲームで言うところの環境とはマップであったり空間であったり，そこに存在するキャラクターであったりしますが，それに作用するとは，プレイヤーの分身であるキャラクターを空間上で動かすこと等に相当します．このエージェントによる行動決定の根拠を方策と言います．強化学習法ではエージェントを成長させますが，これに与えるものは入力データと教師データの組み合わせではありません．エージェントは行動を選択した際のその行動の良さの指標である報酬という値を獲得します．エージェントは自身の行動に基づいてこの報酬を得ますが，環境の遷移が終わったとき（ゲームをクリアしたときや対戦の決着がついたとき）の報酬を最大化するように成長させられます．\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8ViTfSl3gha"
   },
   "source": [
    "```{note}\n",
    "環境は遷移します．例えば，囲碁というゲームでは碁をプレイヤーが動かす度に盤面が変化しますが，この盤面が別の盤面に変化することを環境の遷移と言います．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvfR_cQ6dV83"
   },
   "source": [
    "### 用語の整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYuHql_TdYzC"
   },
   "source": [
    "強化学習法ではこれまでの機械学習法では用いなかった用語を利用するため，それについての整理を行います．\n",
    "\n",
    "**エージェント**とは強化学習法で成長させられる対象です．以下で紹介する Q 学習において成長させられるものは Q テーブルというものであるため厳密には違うのですが，エージェントはそれを参照して行動を決定するため，このように書きました．\n",
    "\n",
    "**環境**とはエージェントが作用を及ぼす対象です．囲碁でいうところの盤面と碁が環境です．任天堂のマリオを動かすゲームでいうところのフィールドやマリオ等が環境です．この環境はエージェントが選択した行動によって遷移をします．\n",
    "\n",
    "**報酬**とはエージェントを成長させるために与える得点です．何かの行動を選択した際に，その行動が良かったなら報酬をエージェントに与えます．エージェントは獲得できる報酬を最大化するように成長します．\n",
    "\n",
    "**方策**とはエージェントが行動を選択するために利用するルールです．これに基づいてエージェントは行動を決定します．\n",
    "\n",
    "**エピソード**とはエージェントが環境に作用し環境の遷移が何らかの停止条件に到達し終了するまでの時間の単位です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isuh1_iiygT0"
   },
   "source": [
    "### 強化学習法の種類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtIMcWPodXRj"
   },
   "source": [
    "強化学習法を実行する方法には様々な分類の方法があります．モデルに基づく方法か価値または方策に基づく方法か等の分類方法です．強化学習法の種類は現在開発が活発であり，どの方法がどのような分類となるかを整理して言及することをこの教材では避けます．\n",
    "\n",
    "最初に，モデルに基づく方法かそうでないかの観点から分類をすると，**動的計画法**はモデルに基づく方法です．動的計画法は正確な環境モデルを持っている場合に最適な方策を決定することが可能な方法です．エピソードを終了せずとも方策を最適化しようとすることが可能です．これ以降で紹介する方法はモデルに基づく方法ではありません．\n",
    "\n",
    "モデルフリーの方法をさらに分類すると，価値ベース法，方策ベース法，アクター・クリティック法の 3 個に大別されます．\n",
    "\n",
    "モデルフリーの方法であり，価値ベース法としては**モンテカルロ法**があります．モンテカルロ法はモデルに基づく方法である動的計画法とは異なり，完全な環境モデルを持たない場合，行動の結果として報酬が得られるかどうかが不明な際に用いられる方法です．エージェントの行動に応じて学習を進めます．エピソードを終了させることではじめて報酬を計算します．これを実績ベースであると言います．\n",
    "\n",
    "モデルフリーの方法であり，価値ベースの方法であり，実績ベースでない方法としては**時間的差分法**（Temporal Difference（TD）法）があります．時間的差分法は上述のふたつを組み合わせたような方法です．行動に対して得られる実際の報酬から評価関数を改善することで最適な方策を見つけようとする方法です．この 3 個の方法の中では時間的差分法が最も現実的な学習法であると思います．時間的差分法はさらに，アルゴリズムの実装の違いにより細分化されます．エージェントの行動を決定する方策と評価関数の更新に用いる方策が異なる方策オフ型の方法である **Q 学習法**と，エージェントの行動を決定する方策と評価関数の更新に用いる方策が同じ方策オン型の方法である**SARSA**です．この章で紹介する方法は Q 学習です．\n",
    "\n",
    "モデルフリーの方法であり，方策ベースの方法としては**方策勾配法**という方法があります．また，アクター・クリティック法とは上述のモデルフリーの方法であり，時間的差分法の一種であるのですが，価値ベースの方法ではないもののことを言います．アクター・クリティック法ではアクターという存在が行動を選択しますが，それを評価するものとしてクリティックという存在があり，アクターの行動をクリティックが評価することで方策を決定する方法です．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAMpHf2QKS0u"
   },
   "source": [
    "```{note}\n",
    "方策ベース法とは，エージェントが何らかの方策（ニューラルネットワークによって記述されるルールとか）を持っており，その方法を改善する方法です．価値ベース法とは，行動から価値を得て（価値関数の値を推定して），その価値を高める方策を選択する方法です．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGZDIZS1oEWh"
   },
   "source": [
    "```{hint}\n",
    "より詳しく学びたい人は Bellman 方程式について調べてみてください．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UZa9cv-SadV"
   },
   "source": [
    "```{note}\n",
    "深層学習の時代において開発が進められている方法は，時間的差分法に分類される Q 学習（価値ベース）と同じくアクター・クリティック法です．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlzVk1zjApJD"
   },
   "source": [
    "## Q 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VofDRr_z0ulT"
   },
   "source": [
    "この節ではフィールド上を動いてゴールを目指すオブジェクトの動きをコントロールするというタスクを通じて Q 学習の利用方法を理解します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3cs3i_60jhA"
   },
   "source": [
    "### 解きたい問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qq_lMLEAtvU"
   },
   "source": [
    "以下のような 5 行 4 列の行列を考えます．これはフィールドです．このフィールド上で何らかのオブジェクト（人でも犬でも何でも良いです）を動かしてゴール（G）を目指すこととします．オブジェクトはフィールド上のグリッド線で囲まれたどこかの位置に存在できることとします．このグリッド線で囲まれた位置のことをこれ以降マスと呼びます．オブジェクトは上下左右に動かせるものとします．ただし，フィールドは壁に囲まれているとします．つまり，オブジェクトはこの 5 行 4 列のフィールドの外には出られません．また，フィールドには障害物（X）があり，障害物が存在する位置にオブジェクトは行けないこととします．オブジェクトは最初ゴールでも障害物でもないオブジェクトが移動可能な普通マス（S），座標にして `(4, 0)` に位置しているものとします．このオブジェクトをうまく移動させてゴールまで導くエージェントを作ることをここでの問題とします．\n",
    "\n",
    "<img src=\"https://github.com/yamada-kd/binds-training/blob/main/image/field.svg?raw=1\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCZaZNBQyg8D"
   },
   "source": [
    "```{hint}\n",
    "左上のマスを `(0, 0)` として右下のマスを `(4, 3)` とします．つまり行列の要素の呼び方と同じですね．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Treesr5S1lkA"
   },
   "source": [
    "```{note}\n",
    "この動かす対象であるオブジェクトのことをエージェントとして記載している記事がインターネット上にたくさんあります．例えば，囲碁における碁に相当するような存在だと思いますが，それをエージェントと呼ぶことはないと思います．エージェントとはプレイヤーであって，ゲームで言うところの環境の一部であるキャラクターや碁等のオブジェクトを移動させたりオブジェクトに何かをさせたりする存在であると思います．よってここではオブジェクトとエージェントを明確に使い分けます．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIeWBsQH0zfE"
   },
   "source": [
    "### Q 学習で行うこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYIl_MCT0R5G"
   },
   "source": [
    "強化学習はエージェントがもらえる報酬を最大化するように学習を行います．エージェントがオブジェクトを動かした結果ゴールまでオブジェクトを導けたのであれば多くの報酬を獲得できます．一方で，障害物があるマスにはオブジェクトを移動できませんがその際には報酬が減ります．その他のマスにオブジェクトを移動させる場合は報酬は得られません．\n",
    "\n",
    "このような状況において，Q 学習では報酬を最大化するために参照する Q テーブルなるものを構築します．学習の過程でこの Q テーブルは更新されます．エージェントは良い Q テーブルを参照することによってより良い性能でオブジェクトを的確に動かせるようになります．つまり，Q 学習で成長させられるものは Q テーブルです．Q テーブルには Q 値が格納されます．というよりは Q 値の集合が Q テーブルです．Q 値は環境がある状態 $s$ にあるときに $a$ というアクションをエージェントがとることの良さを示す値です．$Q(s,a)$ と表します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM-aUgbG6AbH"
   },
   "source": [
    "```{hint}\n",
    "例えば，あるマス「`(4, 0)`」という「状態」でエージェントがオブジェクトを「上に移動させる」という「アクション」をとったときの良さを表すものが Q 値です．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CUw3auL6z0W"
   },
   "source": [
    "```{note}\n",
    "Q 学習の Q は quality のイニシャルです．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfdpzE-yCf4L"
   },
   "source": [
    "Q 値の更新式を紹介します．ここでは更新された Q 値を $Q'$ と書きます．Q 値はエージェントがある行動をとって環境が遷移した後に更新されます．この環境遷移前の Q 値を $Q$，状態を $s$，行動を $a$，獲得した報酬を $r$ と書きます．また，環境遷移後の状態を $s'$，環境遷移後にとり得る行動を $a'$ と書きます．このとき，Q 値の更新式は以下のように表されます．\n",
    "\n",
    "$\n",
    "\\displaystyle Q'(s,a)=Q(s,a)+\\alpha(r+\\gamma\\max Q(s',a')-Q(s,a))\n",
    "$\n",
    "\n",
    "このとき，$\\alpha$ は学習率と呼ばれ，0 から 1 の値をとります．また，$\\gamma$ は割引率であり，0 から 1 の値をとります．この割引率は直後の括弧内の値（TD（temporal difference）誤差という値）をどれくらい更新の際に考慮するかという値です．\n",
    "\n",
    "この式において，$\\max Q(s',a')$ は現在の行動によって遷移した状態において取り得るすべての行動（この場合上下左右の 4 個）に対する Q 値の中で最も大きな値のことです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8GkbqGsGoG1"
   },
   "source": [
    "```{hint}\n",
    "TD 誤差の部分を確認していただければわかるように，Q 値の更新式はある状態から次の状態に遷移したとき，最初の状態の Q 値を次の状態の最も大きな Q 値と報酬の和に近づけることを意味しています．例えば，ゴール直前の状態とそのひとつ前の状態を考えたとき，ひとつ前の状態の Q 値はゴール直前の状態に遷移しようとするように値が大きくなります．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FWVThCO313k"
   },
   "source": [
    "### Q 学習の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfEGmLSyQNtx"
   },
   "source": [
    "実際の Q 学習は以下の手順で行われます．以下の項目の括弧内の記述はこの節で扱う問題に関する記述です．\n",
    "\n",
    "1.   環境の初期化（オブジェクトをスタートのマスに置く）．\n",
    "2.   エージェントによる行動選択（オブジェクトを動かす方向である上下左右を選択する）．\n",
    "3.   環境の更新（オブジェクトを動かそうとする）．\n",
    "4.   環境の更新がそれ以上され得るかの終了条件の判定（ゴールに到達したかどうかを確認）．\n",
    "5.   Q テーブルの更新．\n",
    "6.   上記 1 から 5 の繰り返し．\n",
    "\n",
    "繰り返し作業の単位をエピソードと言います．上の 1 から 5 で 1 エピソードの計算です．学習の最中に epsilon-greedy 法という方法を利用して行動選択を行っています．epsilon-greedy 法とは $\\epsilon$ の確率でランダムに行動選択をし，それ以外の $(1-\\epsilon)$ の確率では最も Q 値の高い行動を選択する方法です．\n",
    "\n",
    "以上の Q 学習を実行するためのコードは以下のものです．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgsP9Lh8QTO1"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def main():\n",
    "    env = Environment()\n",
    "    observation = env.reset()\n",
    "    agent = Agent(alpha=0.1, epsilon=0.3, gamma=0.9, actions=np.arange(4), observation=observation)\n",
    "    \n",
    "    for episode in range(1, 50+1):\n",
    "        rewards = []\n",
    "        observation = env.reset() # 環境の初期化．\n",
    "        while True:\n",
    "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
    "            observation, reward, done = env.step(action) # 環境を進める．\n",
    "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
    "            rewards.append(reward)\n",
    "            if done: break\n",
    "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"],\n",
    "                      [\"X\", \"O\", \"O\", \"O\"],\n",
    "                      [\"O\", \"O\", \"X\", \"O\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = None\n",
    "    \n",
    "    # 以下は環境を初期化する関数．\n",
    "    def reset(self):\n",
    "        self.objectPosition = 4, 0\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = 0\n",
    "        return self.objectPosition\n",
    "    \n",
    "    # 以下は環境を進める関数．\n",
    "    def step(self, action):\n",
    "        self.iteration += 1\n",
    "        y, x = self.objectPosition\n",
    "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
    "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
    "        else:\n",
    "            if action == self.actions[\"up\"]:\n",
    "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
    "            elif action == self.actions[\"down\"]:\n",
    "                y += 1\n",
    "            elif action == self.actions[\"left\"]:\n",
    "                x += -1\n",
    "            elif action == self.actions[\"right\"]:\n",
    "                x += 1\n",
    "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
    "            if self.field[y][x] == \"O\":\n",
    "                self.reward = 0\n",
    "            elif self.field[y][x] == \"G\":\n",
    "                self.done = True\n",
    "                self.reward = 100\n",
    "            self.objectPosition = y, x\n",
    "            return self.objectPosition, self.reward, self.done\n",
    "    \n",
    "    # 以下は移動が可能かどうかを判定する関数．\n",
    "    def checkMovable(self, x, y, action):\n",
    "        if action == self.actions[\"up\"]:\n",
    "            y += -1\n",
    "        elif action == self.actions[\"down\"]:\n",
    "            y += 1\n",
    "        elif action == self.actions[\"left\"]:\n",
    "            x += -1\n",
    "        elif action == self.actions[\"right\"]:\n",
    "            x += 1\n",
    "        if y < 0 or y >= len(self.field):\n",
    "            return False\n",
    "        elif x < 0 or x >= len(self.field[0]):\n",
    "            return False\n",
    "        elif self.field[y][x] == \"X\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.observation = str(observation)\n",
    "        self.qValues = {} # Qテーブル\n",
    "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
    "    \n",
    "    # 以下の関数は行動を選択する関数．\n",
    "    def act(self, observation):\n",
    "        self.observation = str(observation)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
    "        else:\n",
    "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
    "        return action\n",
    "    \n",
    "    # 以下はQテーブルを更新する関数．\n",
    "    def update(self, objectNewPosition, action, reward):\n",
    "        objectNewPosition = str(objectNewPosition)\n",
    "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
    "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
    "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
    "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
    "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZpHl3AlpmSN"
   },
   "source": [
    "実行した結果，エピソードを経るに従ってゴールに到達するまでに要した環境の更新数（`number of steps`）が小さくなり，その更新回数に渡って平均した報酬値（`mean reward`）が大きくなったことがわかります．つまり，学習（Q テーブルの更新）がうまく進みエージェントが成長したことがわかります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEVXuPsEix6f"
   },
   "source": [
    "```{note}\n",
    "この出力だけ見るとこのプログラムを実行して何が起こったのかわかりませんね．次の項でオブジェクトがフィールドをどのように動いたかを可視化します．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I4LFJtXJSqD"
   },
   "source": [
    "プログラム最初から順に説明しますが，以下の部分では環境（マスとか移動するオブジェクトとかそのオブジェクトの位置とか）のインスタンスを生成します．その後，生成した環境を初期化します．次に環境中のオブジェクトを操作するエージェントのインスタンスを生成します．\n",
    "\n",
    "```python\n",
    "    env = Environment()\n",
    "    observation = env.reset()\n",
    "    agent = Agent(alpha=0.1, epsilon=0.3, gamma=0.9, actions=np.arange(4), observation=observation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yHPY6HcwrOL"
   },
   "source": [
    "環境のクラス `Environment` は以下に示す通りです．以降で中身の要素の説明をします．\n",
    "\n",
    "```python\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"],\n",
    "                      [\"X\", \"O\", \"O\", \"O\"],\n",
    "                      [\"O\", \"O\", \"X\", \"O\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = None\n",
    "    \n",
    "    # 以下は環境を初期化する関数．\n",
    "    def reset(self):\n",
    "        self.objectPosition = 4, 0\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = 0\n",
    "        return self.objectPosition\n",
    "    \n",
    "    # 以下は環境を進める関数．\n",
    "    def step(self, action):\n",
    "        self.iteration += 1\n",
    "        y, x = self.objectPosition\n",
    "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
    "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
    "        else:\n",
    "            if action == self.actions[\"up\"]:\n",
    "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
    "            elif action == self.actions[\"down\"]:\n",
    "                y += 1\n",
    "            elif action == self.actions[\"left\"]:\n",
    "                x += -1\n",
    "            elif action == self.actions[\"right\"]:\n",
    "                x += 1\n",
    "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
    "            if self.field[y][x] == \"O\":\n",
    "                self.reward = 0\n",
    "            elif self.field[y][x] == \"G\":\n",
    "                self.done = True\n",
    "                self.reward = 100\n",
    "            self.objectPosition = y, x\n",
    "            return self.objectPosition, self.reward, self.done\n",
    "    \n",
    "    # 以下は移動が可能かどうかを判定する関数．\n",
    "    def checkMovable(self, x, y, action):\n",
    "        if action == self.actions[\"up\"]:\n",
    "            y += -1\n",
    "        elif action == self.actions[\"down\"]:\n",
    "            y += 1\n",
    "        elif action == self.actions[\"left\"]:\n",
    "            x += -1\n",
    "        elif action == self.actions[\"right\"]:\n",
    "            x += 1\n",
    "        if y < 0 or y >= len(self.field):\n",
    "            return False\n",
    "        elif x < 0 or x >= len(self.field[0]):\n",
    "            return False\n",
    "        elif self.field[y][x] == \"X\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1-CUjc1xDgi"
   },
   "source": [
    "以下の部分は環境の初期化をする記述です．最初に `self.actions` ですが，この環境が取りするコマンドはオブジェクトを上下左右に動かすためのもので，その記述です．次の `self.field` ですが，これは上に画像で示したフィールドを生成するためのものです．`X` が障害物，`G` がゴール，それ以外の `O` がオブジェクトが自由に移動できるマスです．`self.done` はオブジェクトがゴールに到達することでこのゲーム（オブジェクトをゴールまで動かすゲーム）が終了したかどうかを判定するための変数です．`self.reward` は報酬を格納する変数です．実はこの `self.iteration` はこのプログラムでは使わないのですが，後のレンダリングの際に必要なので加えています．\n",
    "\n",
    "```python\n",
    "    def __init__(self):\n",
    "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"],\n",
    "                      [\"X\", \"O\", \"O\", \"O\"],\n",
    "                      [\"O\", \"O\", \"X\", \"O\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls0HVol2zw49"
   },
   "source": [
    "```{note}\n",
    "`self.reward` はインスタンス変数にしなくても良かったかもしれません．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l9g3-bsxFTc"
   },
   "source": [
    "環境はエピソード毎にリセットする必要がありますが，そのための記述です．オブジェクトは `(4, 0)` のマスに置きます．このメソッドは戻り値としてオブジェクトの位置を返します．\n",
    "\n",
    "```python\n",
    "    # 以下は環境を初期化する関数．\n",
    "    def reset(self):\n",
    "        self.objectPosition = 4, 0\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = 0\n",
    "        return self.objectPosition\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4lDdPQuxG49"
   },
   "source": [
    "以下は環境を進めるための記述です．最初に `self.checkMovable` でオブジェクトを移動させることができるかを判定します．オブジェクトは壁の外に移動させることができないし，また，障害物のあるマスには移動させることができません．そのような場合は，報酬としては `-1` の値を与えて，また，オブジェクトの存在するマスを変化させません．それ以外の場合は，入力された上下左右のコマンド（`action`）に従ってオブジェクトの位置を変化させます．さらに，報酬は障害物やゴール以外のマスにオブジェクトが位置する場合は `0` でゴールの場合は `100` を与えるようにします．ゴールにオブジェクトが到達している場合は `self.done` に `True` を入れます．\n",
    "\n",
    "```python\n",
    "    # 以下は環境を進める関数．\n",
    "    def step(self, action):\n",
    "        self.iteration += 1\n",
    "        y, x = self.objectPosition\n",
    "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
    "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
    "        else:\n",
    "            if action == self.actions[\"up\"]:\n",
    "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
    "            elif action == self.actions[\"down\"]:\n",
    "                y += 1\n",
    "            elif action == self.actions[\"left\"]:\n",
    "                x += -1\n",
    "            elif action == self.actions[\"right\"]:\n",
    "                x += 1\n",
    "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
    "            if self.field[y][x] == \"O\":\n",
    "                self.reward = 0\n",
    "            elif self.field[y][x] == \"G\":\n",
    "                self.done = True\n",
    "                self.reward = 100\n",
    "            self.objectPosition = y, x\n",
    "            return self.objectPosition, self.reward, self.done\n",
    "    \n",
    "    # 以下は移動が可能かどうかを判定する関数．\n",
    "    def checkMovable(self, x, y, action):\n",
    "        if action == self.actions[\"up\"]:\n",
    "            y += -1\n",
    "        elif action == self.actions[\"down\"]:\n",
    "            y += 1\n",
    "        elif action == self.actions[\"left\"]:\n",
    "            x += -1\n",
    "        elif action == self.actions[\"right\"]:\n",
    "            x += 1\n",
    "        if y < 0 or y >= len(self.field):\n",
    "            return False\n",
    "        elif x < 0 or x >= len(self.field[0]):\n",
    "            return False\n",
    "        elif self.field[y][x] == \"X\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HROtazzI3L0I"
   },
   "source": [
    "次にエージェントのクラス `Agent` は以下に示す通りです．以降で中身の説明をします．\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.observation = str(observation)\n",
    "        self.qValues = {} # Qテーブル\n",
    "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
    "    \n",
    "    # 以下の関数は行動を選択する関数．\n",
    "    def act(self, observation):\n",
    "        self.observation = str(observation)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
    "        else:\n",
    "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
    "        return action\n",
    "    \n",
    "    # 以下はQテーブルを更新する関数．\n",
    "    def update(self, objectNewPosition, action, reward):\n",
    "        objectNewPosition = str(objectNewPosition)\n",
    "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
    "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
    "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
    "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
    "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bGH8RCcC-Pt"
   },
   "source": [
    "最初の記述は，エージェントが持つ変数を生成するためのものです．`self.alpha` や `self.gamma` は Q 値の更新式で利用するものです．`self.epsilon` は epsilon-greedy 法に利用する値です．`self.observation` は上の Q 値の更新式における `s` に相当するものです．マスの座標です．その後の Q テーブル構築の際のディクショナリのキーとなる値です．これを文字列化します．その次のディクショナリ `self.qValues` が Q テーブルです．ここに，ある状態におけるそれぞれの行動の Q 値を格納し，それを学習の過程で更新します．\n",
    "\n",
    "```python\n",
    "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.observation = str(observation)\n",
    "        self.qValues = {} # Qテーブル\n",
    "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7ksBEIGC_2H"
   },
   "source": [
    "以下の部分はエージェントが行動を選択するための記述です．epsilon-greedy 法を利用しています．確率的にランダムな行動を選択するか，または，これまでの Q テーブルを参照して最も Q 値が高い行動をとります．\n",
    "\n",
    "```python\n",
    "    # 以下の関数は行動を選択する関数．\n",
    "    def act(self, observation):\n",
    "        self.observation = str(observation)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
    "        else:\n",
    "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
    "        return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f60YLzfC8Y_"
   },
   "source": [
    "最後の以下の部分は Q テーブルを更新するための記述です．行動選択によって新たなマスへの移動が行われる場合は新たに Q テーブルのキーを生成します．その後，現在の Q 値を計算（参照）し，また，環境遷移後の最も高い Q 値を示す行動の Q 値の値を計算します．これらは，上述の Q 値の更新式で利用する値です．これらを利用して Q 値を更新します．\n",
    "\n",
    "```python\n",
    "    # 以下はQテーブルを更新する関数．\n",
    "    def update(self, objectNewPosition, action, reward):\n",
    "        objectNewPosition = str(objectNewPosition)\n",
    "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
    "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
    "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
    "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
    "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgX3GSALHpXK"
   },
   "source": [
    "最後に，`main()` に戻って以下の部分の説明をします．エピソードは 50 回分計算します．環境の初期化をした後に，`agent.act()` によってオブジェクトにさせる行動を選択します．その行動を基に `env.step()` で環境を勧めます．引き続いて Q テーブルの更新を行います．もし，オブジェクトがゴールに達している場合はそれ以上のオブジェクトの移動や Q テーブルの更新を停止します．最後に，ゴールに到達するまでに要した環境遷移の回数と各エピソード毎に得られた報酬の平均値を出力します．\n",
    "\n",
    "```python\n",
    "    for episode in range(1, 50+1):\n",
    "        rewards = []\n",
    "        observation = env.reset() # 環境の初期化．\n",
    "        while True:\n",
    "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
    "            observation, reward, done = env.step(action) # 環境を進める．\n",
    "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
    "            rewards.append(reward)\n",
    "            if done: break\n",
    "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saOu-Onk5d-w"
   },
   "source": [
    "### 環境の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4KbHIJHDFk"
   },
   "source": [
    "上のプログラムを実行しただけではエージェントによる行動の選択や環境の遷移によってどのようなことが起こっているのかがよくわかりませんでした．以下のプログラムを動かすとどのように環境が遷移したのかを観察することができます．フィールドの様子を可視化している点以外は上のブログラムと同じものです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mo4S9m7xQS_e"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# 環境をステップ毎に描画するようにしたもの．\n",
    "\n",
    "def main():\n",
    "    env = Environment()\n",
    "    observation = env.reset()\n",
    "    agent = Agent(alpha=0.1, epsilon=0.3, gamma=0.9, actions=np.arange(4), observation=observation)\n",
    "    \n",
    "    for episode in range(1, 50+1):\n",
    "        rewards = []\n",
    "        observation = env.reset() # 環境の初期化．\n",
    "        env.render()\n",
    "        while True:\n",
    "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
    "            observation, reward, done = env.step(action) # 環境を進める．\n",
    "            env.render()\n",
    "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
    "            rewards.append(reward)\n",
    "            if done: break\n",
    "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"],\n",
    "                      [\"X\", \"O\", \"O\", \"O\"],\n",
    "                      [\"O\", \"O\", \"X\", \"O\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = None\n",
    "    \n",
    "    # 以下は環境を初期化する関数．\n",
    "    def reset(self):\n",
    "        self.objectPosition = 4, 0\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = 0\n",
    "        return self.objectPosition\n",
    "    \n",
    "    # 以下は環境を進める関数．\n",
    "    def step(self, action):\n",
    "        self.iteration += 1\n",
    "        y, x = self.objectPosition\n",
    "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
    "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
    "        else:\n",
    "            if action == self.actions[\"up\"]:\n",
    "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
    "            elif action == self.actions[\"down\"]:\n",
    "                y += 1\n",
    "            elif action == self.actions[\"left\"]:\n",
    "                x += -1\n",
    "            elif action == self.actions[\"right\"]:\n",
    "                x += 1\n",
    "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
    "            if self.field[y][x] == \"O\":\n",
    "                self.reward = 0\n",
    "            elif self.field[y][x] == \"G\":\n",
    "                self.done = True\n",
    "                self.reward = 100\n",
    "            self.objectPosition = y, x\n",
    "            return self.objectPosition, self.reward, self.done\n",
    "    \n",
    "    # 以下は移動が可能かどうかを判定する関数．\n",
    "    def checkMovable(self, x, y, action):\n",
    "        if action == self.actions[\"up\"]:\n",
    "            y += -1\n",
    "        elif action == self.actions[\"down\"]:\n",
    "            y += 1\n",
    "        elif action == self.actions[\"left\"]:\n",
    "            x += -1\n",
    "        elif action == self.actions[\"right\"]:\n",
    "            x += 1\n",
    "        if y < 0 or y >= len(self.field):\n",
    "            return False\n",
    "        elif x < 0 or x >= len(self.field[0]):\n",
    "            return False\n",
    "        elif self.field[y][x] == \"X\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    # 以下はフィールドとオブジェクト（8）の様子を可視化する関数．\n",
    "    def render(self):\n",
    "        y, x = self.objectPosition\n",
    "        field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                 [\"O\", \"O\", \"O\", \"O\"],\n",
    "                 [\"X\", \"O\", \"O\", \"O\"],\n",
    "                 [\"O\", \"O\", \"X\", \"O\"],\n",
    "                 [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        field[y][x] = \"8\"\n",
    "        print(\"Iteration = {:3d}\".format(self.iteration))\n",
    "        for i in range(5):\n",
    "            for j in range(4):\n",
    "                print(field[i][j], end=\" \")\n",
    "            print()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.observation = str(observation)\n",
    "        self.qValues = {} # Qテーブル\n",
    "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
    "    \n",
    "    # 以下の関数は行動を選択する関数．\n",
    "    def act(self, observation):\n",
    "        self.observation = str(observation)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
    "        else:\n",
    "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
    "        return action\n",
    "    \n",
    "    # 以下はQテーブルを更新する関数．\n",
    "    def update(self, objectNewPosition, action, reward):\n",
    "        objectNewPosition = str(objectNewPosition)\n",
    "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
    "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
    "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
    "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
    "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TPevxmTRguD"
   },
   "source": [
    "プログラムを実行した結果，フィールドが表示され，環境遷移のイタレーションの度に `8` で表示されるオブジェクトがスタート位置からゴール位置へと移動している様子が可視化されました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi5MUslnSBVk"
   },
   "source": [
    "```{note}\n",
    "エピソードの数値が小さいとき，つまり，Q テーブルの性能が低い場合は `8` はフラフラしていますね．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSDtwYfbSEq_"
   },
   "source": [
    "このプログラムが上のプログラムと変わっている点は環境のレンダリングをしているところです．プログラムでは以下の部分に `env.render()` という記述が加わっています．\n",
    "\n",
    "```python\n",
    "    for episode in range(1, 50+1):\n",
    "        rewards = []\n",
    "        observation = env.reset() # 環境の初期化．\n",
    "        env.render()\n",
    "        while True:\n",
    "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
    "            observation, reward, done = env.step(action) # 環境を進める．\n",
    "            env.render()\n",
    "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
    "            rewards.append(reward)\n",
    "            if done: break\n",
    "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RV0amosSf2v"
   },
   "source": [
    "クラス `Environment` の部分に，`env.render()` がしていることの記述があります．フィールドにオブジェクトの位置を代入してそれを表示するだけですね．\n",
    "\n",
    "```\n",
    "    # 以下はフィールドとオブジェクト（8）の様子を可視化する関数．\n",
    "    def render(self):\n",
    "        y, x = self.objectPosition\n",
    "        field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                 [\"O\", \"O\", \"O\", \"O\"],\n",
    "                 [\"X\", \"O\", \"O\", \"O\"],\n",
    "                 [\"O\", \"O\", \"X\", \"O\"],\n",
    "                 [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        field[y][x] = \"8\"\n",
    "        print(\"Iteration = {:3d}\".format(self.iteration))\n",
    "        for i in range(5):\n",
    "            for j in range(4):\n",
    "                print(field[i][j], end=\" \")\n",
    "            print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQQnFSok1Dcc"
   },
   "source": [
    "### Q テーブルの出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCma89PIS4Ks"
   },
   "source": [
    "最後に Q テーブルがどんなように成長したのかを以下のプログラムで確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGRPFR63S8eD"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Qテーブルを出力可能にしたもの．\n",
    "\n",
    "def main():\n",
    "    env = Environment()\n",
    "    observation = env.reset()\n",
    "    agent = Agent(alpha=0.1, epsilon=0.3, gamma=0.9, actions=np.arange(4), observation=observation)\n",
    "    \n",
    "    for episode in range(1, 50+1):\n",
    "        rewards = []\n",
    "        observation = env.reset() # 環境の初期化．\n",
    "        env.render()\n",
    "        while True:\n",
    "            action = agent.act(observation) # エージェントによってオブジェクトにさせるアクションを選択する．\n",
    "            observation, reward, done = env.step(action) # 環境を進める．\n",
    "            env.render()\n",
    "            agent.update(observation, action, reward) # Qテーブルの更新．\n",
    "            rewards.append(reward)\n",
    "            if done: break\n",
    "        print(\"Episode: {:3d}, number of steps: {:3d}, mean reward: {:6.3f}\".format(episode, len(rewards), np.mean(rewards)))\n",
    "        agent.outputQTable()\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.actions = {\"up\": 0, \"down\": 1, \"left\": 2, \"right\": 3}\n",
    "        self.field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"],\n",
    "                      [\"X\", \"O\", \"O\", \"O\"],\n",
    "                      [\"O\", \"O\", \"X\", \"O\"],\n",
    "                      [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = None\n",
    "    \n",
    "    # 以下は環境を初期化する関数．\n",
    "    def reset(self):\n",
    "        self.objectPosition = 4, 0\n",
    "        self.done = False\n",
    "        self.reward = None\n",
    "        self.iteration = 0\n",
    "        return self.objectPosition\n",
    "    \n",
    "    # 以下は環境を進める関数．\n",
    "    def step(self, action):\n",
    "        self.iteration += 1\n",
    "        y, x = self.objectPosition\n",
    "        if self.checkMovable(x, y, action) == False: # オブジェクトの移動が可能かどうかを判定．\n",
    "            return self.objectPosition, -1, False # 移動できないときの報酬は-1．\n",
    "        else:\n",
    "            if action == self.actions[\"up\"]:\n",
    "                y += -1 # フィールドと座標の都合上，上への移動の場合は-1をする．\n",
    "            elif action == self.actions[\"down\"]:\n",
    "                y += 1\n",
    "            elif action == self.actions[\"left\"]:\n",
    "                x += -1\n",
    "            elif action == self.actions[\"right\"]:\n",
    "                x += 1\n",
    "            # 以下のifは報酬の計算とオブジェクトがゴールに到達してゲーム終了となるかどうかの判定のため．\n",
    "            if self.field[y][x] == \"O\":\n",
    "                self.reward = 0\n",
    "            elif self.field[y][x] == \"G\":\n",
    "                self.done = True\n",
    "                self.reward = 100\n",
    "            self.objectPosition = y, x\n",
    "            return self.objectPosition, self.reward, self.done\n",
    "    \n",
    "    # 以下は移動が可能かどうかを判定する関数．\n",
    "    def checkMovable(self, x, y, action):\n",
    "        if action == self.actions[\"up\"]:\n",
    "            y += -1\n",
    "        elif action == self.actions[\"down\"]:\n",
    "            y += 1\n",
    "        elif action == self.actions[\"left\"]:\n",
    "            x += -1\n",
    "        elif action == self.actions[\"right\"]:\n",
    "            x += 1\n",
    "        if y < 0 or y >= len(self.field):\n",
    "            return False\n",
    "        elif x < 0 or x >= len(self.field[0]):\n",
    "            return False\n",
    "        elif self.field[y][x] == \"X\":\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    # 以下はフィールドとオブジェクト（8）の様子を可視化する関数．\n",
    "    def render(self):\n",
    "        y, x = self.objectPosition\n",
    "        field = [[\"X\", \"X\", \"O\", \"G\"],\n",
    "                 [\"O\", \"O\", \"O\", \"O\"],\n",
    "                 [\"X\", \"O\", \"O\", \"O\"],\n",
    "                 [\"O\", \"O\", \"X\", \"O\"],\n",
    "                 [\"O\", \"O\", \"O\", \"O\"]]\n",
    "        field[y][x] = \"8\"\n",
    "        print(\"Iteration = {:3d}\".format(self.iteration))\n",
    "        for i in range(5):\n",
    "            for j in range(4):\n",
    "                print(field[i][j], end=\" \")\n",
    "            print()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha=0.1, epsilon=0.3, gamma=0.9, actions=None, observation=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.observation = str(observation)\n",
    "        self.qValues = {} # Qテーブル\n",
    "        self.qValues[self.observation] = np.repeat(0.0, len(self.actions))\n",
    "    \n",
    "    # 以下の関数は行動を選択する関数．\n",
    "    def act(self, observation):\n",
    "        self.observation = str(observation)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(0, len(self.actions)) # イプシロンの確率でランダムに行動する．\n",
    "        else:\n",
    "            action = np.argmax(self.qValues[self.observation]) # 最もQ値が高い行動を選択．\n",
    "        return action\n",
    "    \n",
    "    # 以下はQテーブルを更新する関数．\n",
    "    def update(self, objectNewPosition, action, reward):\n",
    "        objectNewPosition = str(objectNewPosition)\n",
    "        if objectNewPosition not in self.qValues: # Qテーブルのキーを新たに作る．\n",
    "            self.qValues[objectNewPosition] = np.repeat(0.0, len(self.actions))\n",
    "        q = self.qValues[self.observation][action]  # Q(s,a)の計算．\n",
    "        maxQ = max(self.qValues[objectNewPosition])  # max(Q(s',a'))の計算．\n",
    "        self.qValues[self.observation][action] = q + (self.alpha * (reward + (self.gamma * maxQ) - q)) # Q'(s, a) = Q(s, a) + alpha * (reward + gamma * maxQ(s',a') - Q(s, a))の計算．\n",
    "    \n",
    "    # 以下はQテーブルを出力する関数．\n",
    "    def outputQTable(self):\n",
    "        print(\"Q-table:    Up   Down   Left  Right\")\n",
    "        for key in sorted(self.qValues.keys()):\n",
    "            print(key, end=\" \")\n",
    "            for j in range(4):\n",
    "                print(\"{:7.3f}\".format(self.qValues[key][j]), end=\"\")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOKb7g25TN9p"
   },
   "source": [
    "これは Q テーブルを出力する記述を加えただけなので説明は不要かもしれませんが，Q テーブルを出力するための記述はクラス `Agent` の以下の部分に追加しました．\n",
    "\n",
    "```python\n",
    "    # 以下はQテーブルを出力する関数．\n",
    "    def outputQTable(self):\n",
    "        print(\"Q-table:    Up   Down   Left  Right\")\n",
    "        for key in sorted(self.qValues.keys()):\n",
    "            print(key, end=\" \")\n",
    "            for j in range(4):\n",
    "                print(\"{:7.3f}\".format(self.qValues[key][j]), end=\"\")\n",
    "            print()\n",
    "        print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUryaOsdTFWy"
   },
   "source": [
    "全てのエピソードが終了した後の Q テーブルは以下のようなものとなりました．例えば，ゴール付近のマスである，`(0, 2)` や `(1, 3)` ではどのような行動が高い Q 値を持つかと確認してみると，オブジェクトが `(0, 2)` のときは「右」に移動させることが，`(1, 3)` のときは「上」に移動させることが最も高い Q 値を示していました．良い Q テーブルへと成長したことが確認できます．\n",
    "\n",
    "```\n",
    "Q-table:    Up   Down   Left  Right\n",
    "(0, 2)  10.674  1.824 -0.190 99.030\n",
    "(0, 3)   0.000  0.000  0.000  0.000\n",
    "(1, 0)  -0.100 -0.100 -0.100  7.221\n",
    "(1, 1)  27.443  0.433  0.238 68.646\n",
    "(1, 2)  84.938  7.338 18.591 12.410\n",
    "(1, 3)  46.856  0.000  6.858  1.610\n",
    "(2, 1)  48.270  0.997  9.978 10.193\n",
    "(2, 2)  44.444  0.453  0.000  0.000\n",
    "(3, 0)   1.532  1.509  2.382 20.779\n",
    "(3, 1)  32.544  0.346  0.864  3.671\n",
    "(4, 0)  12.940 -0.254  1.262  0.341\n",
    "(4, 1)   0.457 -0.171  3.350  0.000\n",
    "(4, 2)  -0.271 -0.100  0.041  0.000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqwWY1c0O02R"
   },
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vjEUHVaiDWW"
   },
   "source": [
    "OpenAI Gym を利用すると様々なゲームを Python からコントロールできるようになります．スーパーマリオブラザーズ等の有名なゲームも Python コード上で構築したエージェントが動かすことができるようになります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSKzyoA1O02W"
   },
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R2Wn48-4NwS"
   },
   "source": [
    "### 環境の生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NIFMBsqXNjY"
   },
   "source": [
    "環境の遷移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD4N6qje9TwB"
   },
   "source": [
    "## 深層 Q 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgHioec6iJS-"
   },
   "source": [
    "Q 学習に深層学習法を利用した深層 Q 学習という方法を紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCIkDQK74Q2_"
   },
   "source": [
    "### 深層学習と Q 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfC8Kj03omW"
   },
   "source": [
    "```{note}\n",
    "終わりです．\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
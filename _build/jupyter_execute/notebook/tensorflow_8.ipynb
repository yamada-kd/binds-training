{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_xhvUO10XD6"
   },
   "source": [
    "# 敵対的生成ネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5XKyMs27rU-"
   },
   "source": [
    "```{attention}\n",
    "作成途中！！！！！！！\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Sx06lGrg3K"
   },
   "source": [
    "## 基本的な事柄\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-S6ghDZpmSG"
   },
   "source": [
    "敵対的生成ネットワークとは英語では generative adversarial network（GAN）と言う（主に）ニューラルネットワークを用いてデータを生成する方法です．入力データを変換することもできますが，やっていることは変更したデータを「生成」することであるため「生成する方法です」と書きました．GAN は現実世界の色々なところで既に応用されている技術です．とても重要な技術なのでここで紹介します．最も基本的な GAN を紹介した後に，その GAN の問題点を解決する GAN を紹介して，最後により便利な利用が可能な GAN を紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Utwmr7SlIWLw"
   },
   "source": [
    "```{note}\n",
    "GAN の構造には必ずしもニューラルネットワークが含まれる必要はありません．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EliP-z_krg3N"
   },
   "source": [
    "### 原理と利用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKQO9oRW_yO"
   },
   "source": [
    "生成器，識別器，生成器のコスト関数，識別器のコスト関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3P_sTkDzu4O"
   },
   "source": [
    "トランスフォーマーとはアテンションとポジショナルエンコードといわれる技術を用いて，再帰型ニューラルネットワークとは異なる方法で文字列を処理することができるニューラルネットワークの構造です．機械翻訳や質問応答に利用することができます．\n",
    "\n",
    "例えば，機械翻訳の場合，翻訳したい文字列を入力データ，翻訳結果の文字列を教師データとして利用します．構築した人工知能は翻訳したい文字列を入力値として受け取り，配列を出力します．配列の各要素は文字の個数と同じサイズのベクトル（その要素が何の文字なのかを示す確率ベクトル）です．\n",
    "\n",
    "トランスフォーマーはエンコーダーとデコーダーという構造からなります．エンコーダーは配列（機械翻訳の場合，翻訳したい配列）を入力にして，同じ長さの配列を出力します．デコーダーも配列（機械翻訳の場合，翻訳で得たい配列）とエンコーダーが出力した配列を入力にして同じ長さの配列（各要素は確率ベクトル）を出力します．エンコーダーが出力した配列情報をデコーダーで処理する際にアテンションという技術が利用されます．\n",
    "\n",
    "<img src=\"https://github.com/yamada-kd/binds-training/blob/main/image/gan.svg?raw=1\" width=\"100%\" />\n",
    "\n",
    "エンコーダーとデコーダー間のアテンション以外にも，エンコーダーとデコーダーの内部でもそれぞれアテンション（セルフアテンション）が計算されます．アテンションは文字列内における文字の関連性を計算します．\n",
    "\n",
    "トランスフォーマーは再帰型ニューラルネットワークで行うような文字の逐次的な処理が不要です．よって，計算機の並列化性能をより引き出せます．扱える文脈の長さも無限です（再帰型ニューラルネットワークでも理論上無限です．）．\n",
    "\n",
    "このトランスフォーマーはものすごい性能を発揮しており，これまでに作られてきた様々な構造を過去のものとしました．特に応用の範囲が広いのはトランスフォーマーのエンコーダーの部分です．BERT と呼ばれる方法を筆頭に自然言語からなる配列を入力にして何らかの分散表現を出力する方法として自然言語処理に関わる様々な研究開発に利用されています．\n",
    "\n",
    "（会話でトランスフォーマーという場合は，トランスフォーマーのエンコーダーまたはデコーダーのことを言っている場合があります．エンコーダー・デコーダー，エンコーダー，デコーダー，この3個でそれぞれできることが異なります．）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8ViTfSl3gha"
   },
   "source": [
    "```{hint}\n",
    "実用上，配列を入力にして配列を返す構造とだけ覚えておけば問題はないと思います．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isuh1_iiygT0"
   },
   "source": [
    "### GAN Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtIMcWPodXRj"
   },
   "source": [
    "Hugging Face で扱うことができるタスクは以下に示すものがあります．これ以外にもありますが自然言語処理に関する代表的なタスクを抽出しました．括弧内の文字列は実際に Hugging Face を利用する際に指定するオプションです（後で利用します．）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRGjF8pHdtO1"
   },
   "source": [
    "    \n",
    "\n",
    "*   感情分析（`sentiment-analysis`）：入力した文章が有する感情を予測\n",
    "*   特徴抽出（`feature-extraction`）：入力した文章をその特徴を示すベクトルに変換\n",
    "*   穴埋め（`fill-mask`）：文章中のマスクされた単語を予測\n",
    "*   固有表現抽出（`ner`）：入力した文章中の固有表現（名前とか場所とか）にラベルをつける\n",
    "*   質問応答（`question-answering`）：質問文とその答えが含まれる何らかの説明文を入力として解答文を生成\n",
    "*   要約（`summarization`）：入力した文章を要約\n",
    "*   文章生成（`text-generation`）：文章を入力にして，その文章に続く文章を生成\n",
    "*   翻訳（`translation`）：文章を他の言語に翻訳\n",
    "*   ゼロショット文章分類（`zero-shot-classification`）：文章とそれが属する可能性があるいくつかのカテゴリを入力にしてその文章をひとつのカテゴリに分類\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RlLdfs5ynbW"
   },
   "source": [
    "## 基本的な GAN の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfEGmLSyQNtx"
   },
   "source": [
    "最も基本的な GAN の構造は以下のプログラムに含まれるものです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgsP9Lh8QTO1"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def main():\n",
    "    # ハイパーパラメータの設定\n",
    "    MiniBatchSize = 300\n",
    "    NoiseSize = 100 # GANはランダムなノイズベクトルから何かを生成する方法なので，そのノイズベクトルのサイズを設定する．\n",
    "    MaxEpoch = 500\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    (learnX, learnT), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    learnX = learnX.reshape([60000, 784])\n",
    "    learnX = (learnX - 127.5) / 127.5\n",
    "    \n",
    "    # 生成器と識別器の構築\n",
    "    generator = Generator() # 下のクラスを参照．\n",
    "    discriminator = Discriminator() # 下のクラスを参照．\n",
    "    \n",
    "    # コスト関数と正確度を計算する関数の生成\n",
    "    costComputer = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    accuracyComputer = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # オプティマイザは生成器と識別器で別々のものを利用\n",
    "    optimizerGenerator = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    optimizerDiscriminator = tf.keras.optimizers.Adam(learning_rate=0.00004)\n",
    "    \n",
    "    # 生成器を成長させるためのコストを計算する関数\n",
    "    def generatorCostFunction(discriminatorOutputFromGenerated):\n",
    "        cost = costComputer(tf.ones(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated) # 生成器のコストの引数は生成器の出力を識別器に入れた結果．生成器としては全部正解を出しているはずなので教師はすべて1となるはず．そのように学習すべき．\n",
    "        accuracy = accuracyComputer(tf.ones(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated)\n",
    "        return cost, accuracy\n",
    "    \n",
    "    # 識別器を成長させるためのコストを計算する関数\n",
    "    def discriminatorCostFunction(discriminatorOutputFromReal,discriminatorOutputFromGenerated): # 識別器のコストの引数は本物の情報を識別器に入れた結果と生成器の出力を識別器に入れた結果．\n",
    "        realCost = costComputer(tf.ones(discriminatorOutputFromReal.shape[0]), discriminatorOutputFromReal) # 本物の情報の場合はすべて正例（1）と判断すべき．これが例のコスト関数の左の項に相当．\n",
    "        fakeCost = costComputer(tf.zeros(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated) # 偽物の情報の場合はすべて負例（0）と判断すべき．これが例のコスト関数の右の項に相当．\n",
    "        cost = realCost + fakeCost\n",
    "        realAccuracy = accuracyComputer(tf.ones(discriminatorOutputFromReal.shape[0]), discriminatorOutputFromReal)\n",
    "        fakeAccuracy = accuracyComputer(tf.zeros(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated)\n",
    "        accuracy=(realAccuracy + fakeAccuracy) / 2\n",
    "        return cost, accuracy\n",
    "    \n",
    "    @tf.function()\n",
    "    def run(generator, discriminator, noiseVector, realVector):\n",
    "        with tf.GradientTape() as generatorTape, tf.GradientTape() as discriminatorTape:\n",
    "            generatedVector = generator(noiseVector) # 生成器によるデータの生成．\n",
    "            discriminatorOutputFromGenerated = discriminator(generatedVector) # その生成データを識別器に入れる．\n",
    "            discriminatorOutputFromReal = discriminator(realVector) # 本物データを識別器に入れる．\n",
    "            # 識別器の成長\n",
    "            discriminatorCost, discriminatorAccuracy = discriminatorCostFunction(discriminatorOutputFromReal, discriminatorOutputFromGenerated)\n",
    "            gradientDiscriminator = discriminatorTape.gradient(discriminatorCost, discriminator.trainable_variables) # 識別器のパラメータだけで勾配を計算．つまり生成器のパラメータは行わない．\n",
    "            optimizerDiscriminator.apply_gradients(zip(gradientDiscriminator, discriminator.trainable_variables))\n",
    "            # 生成器の成長\n",
    "            generatorCost, generatorAccuracy = generatorCostFunction(discriminatorOutputFromGenerated)\n",
    "            gradientGenerator = generatorTape.gradient(generatorCost,generator.trainable_variables) # 生成器のパラメータで勾配を計算．\n",
    "            optimizerGenerator.apply_gradients(zip(gradientGenerator,generator.trainable_variables))\n",
    "            return discriminatorCost, discriminatorAccuracy, generatorCost, generatorAccuracy\n",
    "    \n",
    "    # ミニバッチセットの生成\n",
    "    learnX = tf.data.Dataset.from_tensor_slices(learnX) # このような方法を使うと簡単にミニバッチを実装することが可能．\n",
    "    learnT = tf.data.Dataset.from_tensor_slices(learnT)\n",
    "    learnA = tf.data.Dataset.zip((learnX, learnT)).shuffle(60000).batch(MiniBatchSize) # 今回はインプットデータしか使わないけど後にターゲットデータを使う場合があるため．\n",
    "    miniBatchNumber = len(list(learnA.as_numpy_iterator()))\n",
    "    # 学習ループ\n",
    "    for epoch in range(1,MaxEpoch+1):\n",
    "        discriminatorCost, discriminatorAccuracy, generatorCost, generatorAccuracy = 0, 0, 0, 0\n",
    "        for learnx, _ in learnA:\n",
    "            noiseVector = generateNoise(MiniBatchSize, NoiseSize) # ミニバッチサイズで100個の要素からなるノイズベクトルを生成．\n",
    "            discriminatorCostTmp, discriminatorAccuracyTmp, generatorCostTmp, generatorAccuracyTmp = run(generator, discriminator, noiseVector, learnx)\n",
    "            discriminatorCost += discriminatorCostTmp / miniBatchNumber\n",
    "            discriminatorAccuracy += discriminatorAccuracyTmp / miniBatchNumber\n",
    "            generatorCost += generatorCostTmp / miniBatchNumber\n",
    "            generatorAccuracy += generatorAccuracyTmp / miniBatchNumber\n",
    "        # 疑似的なテスト\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch {:10d} D-cost {:6.4f} D-acc {:6.4f} G-cost {:6.4f} G-acc {:6.4f} \".format(epoch,float(discriminatorCost),float(discriminatorAccuracy),float(generatorCost),float(generatorAccuracy)))\n",
    "            validationNoiseVector = generateNoise(1, NoiseSize)\n",
    "            validationOutput = generator(validationNoiseVector)\n",
    "            validationOutput = np.asarray(validationOutput).reshape([1, 28, 28])\n",
    "            plt.imshow(validationOutput[0], cmap = \"gray\")\n",
    "            plt.pause(1)\n",
    "\n",
    "# 入力されたデータを0か1に分類するネットワーク\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(units=128)\n",
    "        self.d2 = tf.keras.layers.Dense(units=128)\n",
    "        self.d3 = tf.keras.layers.Dense(units=128)\n",
    "        self.d4 = tf.keras.layers.Dense(units=2, activation=\"softmax\")\n",
    "        self.a = tf.keras.layers.LeakyReLU()\n",
    "        self.d = tf.keras.layers.Dropout(0.5)\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.d(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.d(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.a(y)\n",
    "        y = self.d(y)\n",
    "        y = self.d4(y)\n",
    "        return y\n",
    "\n",
    "# 入力されたベクトルから別のベクトルを生成するネットワーク\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.d1=tf.keras.layers.Dense(units=256)\n",
    "        self.d2=tf.keras.layers.Dense(units=256)\n",
    "        self.d3=tf.keras.layers.Dense(units=784)\n",
    "        self.a=tf.keras.layers.LeakyReLU()\n",
    "        self.b1=tf.keras.layers.BatchNormalization()\n",
    "        self.b2=tf.keras.layers.BatchNormalization()\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.b1(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.b2(y)\n",
    "        y = self.d3(y)\n",
    "        return y\n",
    "\n",
    "def generateNoise(miniBatchSize, randomNoiseSize):\n",
    "    return np.random.uniform(-1,1,size=(miniBatchSize,randomNoiseSize)).astype(\"float32\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZpHl3AlpmSN"
   },
   "source": [
    "とても簡単に自然言語処理を実現することができる利用方法を紹介します．世界最高性能を求めたいとかでないなら，ここで紹介する方法を利用して様々なことを達成できると思います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saOu-Onk5d-w"
   },
   "source": [
    "### 感情分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_RY-XjT5d-w"
   },
   "source": [
    "最も簡単な `tranformers` の利用方法は以下のようになると思います．`pipeline` を読み込んで，そこに取り組みたいタスク（`sentiment-analysis`）を指定します．初回の起動の際には事前学習済みモデルがダウンロードされるため時間がかかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4QgsbGyfBA3"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    text = \"I have a pen.\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4KbHIJHDFk"
   },
   "source": [
    "入力した文章がポジティブな文章なのかネガティブな文章なのかを分類できます．ここでは1個の文章を入力しましたが，以下のように2個以上の文章も入力可能です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVrVwjvFHVax"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    litext = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    "    result = classifier(litext)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqwWY1c0O02R"
   },
   "source": [
    "## WGAN-gp の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pjsr5ACupmST"
   },
   "source": [
    "これまでに利用したものとは異なるモデルを利用したいとか，自身が持っているデータセットにより適合させたいとかの応用的な利用方法を紹介します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-qwkX3SCCRC"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def main():\n",
    "    # ハイパーパラメータの設定\n",
    "    MiniBatchSize = 300\n",
    "    NoiseSize = 100 # GANはランダムなノイズベクトルから何かを生成する方法なので，そのノイズベクトルのサイズを設定する．\n",
    "    MaxEpoch = 300\n",
    "    CriticLearningNumber = 5\n",
    "    GradientPenaltyCoefficient = 10\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    (learnX, learnT), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    learnX = np.asarray(learnX.reshape([60000, 784]), dtype=\"float32\")\n",
    "    learnX = (learnX - 127.5) / 127.5\n",
    "    \n",
    "    # 生成器と識別器の構築\n",
    "    generator = Generator() # 下のクラスを参照．\n",
    "    critic = Critic() # 下のクラスを参照．\n",
    "    \n",
    "    # オプティマイザは生成器と識別器で同じで良い．が，ハイパーパラメータを変えたくなるかもしれないからふたつ用意．\n",
    "    optimizerGenerator = tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0,beta_2=0.9)\n",
    "    optimizerCritic = tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0,beta_2=0.9)\n",
    "    \n",
    "    @tf.function()\n",
    "    def runCritic(generator, critic, noiseVector, realVector):\n",
    "        with tf.GradientTape() as criticTape:\n",
    "            generatedVector = generator(noiseVector) # 生成器によるデータの生成．\n",
    "            criticOutputFromGenerated = critic(generatedVector) # その生成データを識別器に入れる．\n",
    "            criticOutputFromReal = critic(realVector) # 本物データを識別器に入れる．\n",
    "            epsilon = tf.random.uniform(generatedVector.shape, minval=0, maxval=1)\n",
    "            intermediateVector = generatedVector + epsilon * (realVector - generatedVector)\n",
    "            # 勾配ペナルティ\n",
    "            with tf.GradientTape() as gradientPenaltyTape:\n",
    "                gradientPenaltyTape.watch(intermediateVector)\n",
    "                criticOutputFromIntermediate = critic(intermediateVector)\n",
    "                gradientVector = gradientPenaltyTape.gradient(criticOutputFromIntermediate, intermediateVector)\n",
    "                gradientNorm = tf.norm(gradientVector, ord=\"euclidean\", axis=1) # gradientNorm = tf.sqrt(tf.reduce_sum(tf.square(gradientVector), axis=1)) と書いても良い．\n",
    "                gradientPenalty = (gradientNorm - 1)**2\n",
    "            # 識別器の成長\n",
    "            criticCost = tf.reduce_mean(criticOutputFromGenerated - criticOutputFromReal + GradientPenaltyCoefficient * gradientPenalty) # 識別器を成長させるためのコストを計算．WGANの元論文の式そのまま．\n",
    "            gradientCritic = criticTape.gradient(criticCost, critic.trainable_variables) # 識別器のパラメータだけで勾配を計算．つまり生成器のパラメータは行わない．\n",
    "            optimizerCritic.apply_gradients(zip(gradientCritic, critic.trainable_variables))\n",
    "            return criticCost\n",
    "    \n",
    "    @tf.function()\n",
    "    def runGenerator(generator, critic, noiseVector):\n",
    "        with tf.GradientTape() as generatorTape:\n",
    "            generatedVector = generator(noiseVector) # 生成器によるデータの生成．\n",
    "            criticOutputFromGenerated = critic(generatedVector) # その生成データを識別器に入れる．\n",
    "            # 生成器の成長\n",
    "            generatorCost = -tf.reduce_mean(criticOutputFromGenerated) # 生成器を成長させるためのコストを計算．\n",
    "            gradientGenerator = generatorTape.gradient(generatorCost,generator.trainable_variables) # 生成器のパラメータで勾配を計算．\n",
    "            optimizerGenerator.apply_gradients(zip(gradientGenerator,generator.trainable_variables))\n",
    "            return generatorCost\n",
    "    \n",
    "    # ミニバッチセットの生成\n",
    "    learnX = tf.data.Dataset.from_tensor_slices(learnX) # このような方法を使うと簡単にミニバッチを実装することが可能．\n",
    "    learnT = tf.data.Dataset.from_tensor_slices(learnT)\n",
    "    learnA = tf.data.Dataset.zip((learnX, learnT)).shuffle(60000).batch(MiniBatchSize) # 今回はインプットデータしか使わないけど後にターゲットデータを使う場合があるため．\n",
    "    miniBatchNumber = len(list(learnA.as_numpy_iterator()))\n",
    "    # 学習ループ\n",
    "    for epoch in range(1,MaxEpoch+1):\n",
    "        criticCost, generatorCost = 0, 0\n",
    "        for learnx, _ in learnA:\n",
    "            # WGAN-gpでは識別器1回に対して生成器を複数回学習させるのでそのためのループ．\n",
    "            for _ in range(CriticLearningNumber):\n",
    "                noiseVector = generateNoise(MiniBatchSize, NoiseSize) # ミニバッチサイズで100個の要素からなるノイズベクトルを生成．\n",
    "                criticCostPiece = runCritic(generator, critic, noiseVector, learnx)\n",
    "                criticCost += criticCostPiece / (CriticLearningNumber * miniBatchNumber)\n",
    "            # WGAN-gpでは識別器1回に対して生成器を複数回学習させるのでそのためのループ．\n",
    "            for _ in range(1):\n",
    "                noiseVector = generateNoise(MiniBatchSize, NoiseSize) # ミニバッチサイズで100個の要素からなるノイズベクトルを生成．\n",
    "                generatorCostPiece = runGenerator(generator, critic, noiseVector)\n",
    "                generatorCost += generatorCostPiece / miniBatchNumber\n",
    "        # 疑似的なテスト\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch {:10d} D-cost {:6.4f} G-cost {:6.4f} \".format(epoch,float(criticCost),float(generatorCost)))\n",
    "            validationNoiseVector = generateNoise(1, NoiseSize)\n",
    "            validationOutput = generator(validationNoiseVector)\n",
    "            validationOutput = np.asarray(validationOutput).reshape([1, 28, 28])\n",
    "            plt.imshow(validationOutput[0], cmap = \"gray\")\n",
    "            plt.pause(1)\n",
    "\n",
    "# 入力されたデータを0か1に分類するネットワーク\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic,self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(units=128)\n",
    "        self.d2 = tf.keras.layers.Dense(units=128)\n",
    "        self.d3 = tf.keras.layers.Dense(units=128)\n",
    "        self.d4 = tf.keras.layers.Dense(units=1)\n",
    "        self.a = tf.keras.layers.LeakyReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.a(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.d4(y)\n",
    "        return y\n",
    "\n",
    "# 入力されたベクトルから別のベクトルを生成するネットワーク\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.d1=tf.keras.layers.Dense(units=128)\n",
    "        self.d2=tf.keras.layers.Dense(units=128)\n",
    "        self.d3=tf.keras.layers.Dense(units=128)\n",
    "        self.d4=tf.keras.layers.Dense(units=784)\n",
    "        self.a=tf.keras.layers.LeakyReLU()\n",
    "        self.b1=tf.keras.layers.BatchNormalization()\n",
    "        self.b2=tf.keras.layers.BatchNormalization()\n",
    "        self.b3=tf.keras.layers.BatchNormalization()\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.b1(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.b2(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.a(y)\n",
    "        y = self.b3(y)\n",
    "        y = self.d4(y)\n",
    "        y = tf.keras.activations.tanh(y)\n",
    "        return y\n",
    "\n",
    "def generateNoise(miniBatchSize, randomNoiseSize):\n",
    "    return np.random.uniform(-1,1,size=(miniBatchSize,randomNoiseSize)).astype(\"float32\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSKzyoA1O02W"
   },
   "source": [
    "### 他のモデルの利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G316vauoido7"
   },
   "source": [
    "これまでに，Hugging Face が自動でダウンロードしてくれたデフォルトの事前学習済みモデルを利用した予測を行いましたが，そうでなくて，モデルを指定することもできます．以下のページをご覧ください．Model Hub と言います．\n",
    "\n",
    "https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "\n",
    "\n",
    "この Model Hub の Tasks というところでタグを選択できます．例えば，Text Generation の `distilgpt2` を利用するには以下のように書きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEduyYGdPhf0"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    text = \"In this course, we will teach you how to\"\n",
    "    result = generator(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD4N6qje9TwB"
   },
   "source": [
    "## Conditional GAN の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfC8Kj03omW"
   },
   "source": [
    "```{note}\n",
    "終わりです．\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_8.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
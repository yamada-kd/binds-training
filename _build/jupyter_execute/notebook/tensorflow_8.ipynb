{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_xhvUO10XD6"
   },
   "source": [
    "# 敵対的生成ネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5XKyMs27rU-"
   },
   "source": [
    "```{attention}\n",
    "作成途中！！！！！！！\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Sx06lGrg3K"
   },
   "source": [
    "## 基本的な事柄\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-S6ghDZpmSG"
   },
   "source": [
    "敵対的生成ネットワークとは英語では generative adversarial network（GAN）と言う（主に）ニューラルネットワークを用いてデータを生成する方法です．入力データを変換することもできますが，やっていることは変更したデータを「生成」することであるため「生成する方法です」と書きました．GAN は現実世界の色々なところで既に応用されている技術です．とても重要な技術なのでここで紹介します．最も基本的な GAN を紹介した後に，その GAN の問題点を解決する GAN を紹介して，最後により便利な利用が可能な GAN を紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Utwmr7SlIWLw"
   },
   "source": [
    "```{note}\n",
    "GAN の構造には必ずしもニューラルネットワークが含まれる必要はありません．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EliP-z_krg3N"
   },
   "source": [
    "### 原理と活用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3P_sTkDzu4O"
   },
   "source": [
    "GAN の構成要素はふたつです．生成器（generator）と識別器（discriminator）です．識別器は以下で紹介する Wasserstein GAN（WGAN）を利用する場合は critic と名前を変えます．これの和訳は多分ありません．ここではクリティックと表記します．GAN はデータを生成（その派生として変換）する人工知能を作る方法です．最終的に得たいのは性能良く育った生成器です．識別器の役割は生成器をより良く育てることです．\n",
    "\n",
    "GAN の学習は，より良い質の偽札を作ろうとする紙幣の偽造者とその偽札を見破る警察によるイタチごっこに例えられます．GAN においては偽造者が生成器で，警察が識別器です．GAN は以下のような構造をしています．生成器にはランダムに発生させられたノイズデータが入力されます．このランダムノイズを入力にして生成器は質の良い偽札を生成しようとします．これに対して，識別器には生成器から出力された偽札データ（偽物のデータ）または，本物の札（本物のデータ）のどちらかが入力されます．このどちらかのデータを入力にして識別器は入力データが本物か偽物かの値を出力します．例えば，生成したいデータが画像である場合，生成器から出力されるものは画像で，識別器から出力されるものは `0` または `1` です．\n",
    "\n",
    "<img src=\"https://github.com/yamada-kd/binds-training/blob/main/image/gan.svg?raw=1\" width=\"100%\" />\n",
    "\n",
    "学習の最中に，生成器の性能と識別器の性能がどちらも良くなるようにそれらを育てます．よって生成器が良い感じに成長するとより質の高い偽札が生成されます．これに対して，識別器も偽札と本物の札を見分ける能力が高まるので，生成器は識別器に見破られないためにはさらに性能を向上させなければなりません．GAN の学習ではこのようなイタチごっこを行うことで生成器の性能を極限まで高めようとしています．\n",
    "\n",
    "GAN の学習においては生成器と識別器のどちらもを成長させなければなりません．具体的には以下のようなふたつのパラメータ更新を別々に行います．\n",
    "\n",
    "*   生成器を成長させる際には，ノイズを入力にして出力された偽物のデータを識別器の入力として識別器が真偽を識別した結果得られる出力に対して計算したコストから生成器のパラメータについて勾配を計算し，これを用いて生成器だけのパラメータの更新を行います．\n",
    "*   識別器を成長させる際には，生成器から得られた偽物のデータか本物のデータを入力として識別器が真偽を識別した結果得られる出力に対して計算したコストから識別器のパラメータについて勾配を計算し，これを用いて識別器だけのパラメータ更新を行います．\n",
    "\n",
    "生成器のコスト関数を $P$，識別器のコスト関数を $Q$ で表したとき，生成器を成長させるためのコストを計算するコスト関数は以下のように表されます．\n",
    "\n",
    "$\n",
    "\\displaystyle P(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_{i=1}^{N}\\log(1-D(\\boldsymbol{\\phi},G(\\boldsymbol{\\theta},\\boldsymbol{z}_i)))\n",
    "$\n",
    "\n",
    "ここで，生成器は $G$，識別器は $D$ で表しています．それぞれのニューラルネットワークのパラメータは $\\boldsymbol{\\theta}$ と $\\boldsymbol{\\phi}$ で，また，生成器と識別器へ入力されたデータの個数（サイズ）は $N$ です．生成器への入力データである $N$ 個のノイズの $i$ 番目のデータを $\\boldsymbol{z}_i$ と表し，識別器への $i$ 番目の入力データを $\\boldsymbol{x}_i$ と表記しています．識別器は入力されたデータが偽である場合 `0` を出力し，真である場合 `1` を出力するものとします．この場合，生成器の出力は全て真であると識別してほしいので，この $P$ を小さくすれば目的に適うわけです．一方で，識別器のコスト関数は以下のように表されます．\n",
    "\n",
    "$\n",
    "\\displaystyle Q(\\boldsymbol{\\phi})=\\frac{1}{N}\\sum_{i=1}^{N}(\\log D(\\boldsymbol{\\phi},\\boldsymbol{x}_i)+\\log(1-D(\\boldsymbol{\\phi},G(\\boldsymbol{\\theta},\\boldsymbol{z}_i))))\n",
    "$\n",
    "\n",
    "本物のデータに対しては `0` を出力してほしいし，偽物のデータに対しては `1` を出力してほしいので，この $Q$ を小さくすれば良いのですね．\n",
    "\n",
    "ここでは，このように生成器と識別器のコスト関数を分けて書きましたが，元の論文には以下のように書かれています．\n",
    "\n",
    "$\n",
    "\\displaystyle \\min_G \\max_D V(D,G) = \\min_G \\max_D \\mathbb E_{x \\sim p_{data}(x)}[\\log{D(x)}]+ \\mathbb E_{z \\sim p_z(z)}[\\log{(1-D(G(z)))}]\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8ViTfSl3gha"
   },
   "source": [
    "```{note}\n",
    "元論文の式，これ厳密でしょうか？これだけ見せられたら意味わからないです．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isuh1_iiygT0"
   },
   "source": [
    "### 色々な GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtIMcWPodXRj"
   },
   "source": [
    "GAN は色々なところで使われています．元々の GAN は2014年に開発されたのですが，その後もその改良は様々な研究者によって行われてきました．GAN の特有の問題を解決するための研究や GAN の適用先を画像解析分野や文字列解析分野にした研究や GAN に新たな機能を加える研究等です．数多くの GAN が開発されており，それらの GAN をまとめて GAN Zoo という場合があります．GAN Zoo は以下のウェブサイトにまとめられています．\n",
    "\n",
    "https://github.com/hindupuravinash/the-gan-zoo\n",
    "\n",
    "全ての GAN について説明することはできないため，ここでは，最も基本的な GAN を紹介した後に，GAN の特有の問題であるモード崩壊を抑制するために開発された方法である WGAN-gp と GAN の入力に何らかの条件を加えることで，その条件にあった出力をさせることができるようになる Conditional GAN（CGAN）の紹介を行います．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RlLdfs5ynbW"
   },
   "source": [
    "## 基本的な GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FWVThCO313k"
   },
   "source": [
    "### GAN の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfEGmLSyQNtx"
   },
   "source": [
    "最も基本的な GAN の構造は以下のプログラムに含まれるものです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgsP9Lh8QTO1"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def main():\n",
    "    # ハイパーパラメータの設定\n",
    "    MiniBatchSize = 300\n",
    "    NoiseSize = 100 # GANはランダムなノイズベクトルから何かを生成する方法なので，そのノイズベクトルのサイズを設定する．\n",
    "    MaxEpoch = 500\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    (learnX, learnT), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    learnX = learnX.reshape([60000, 784])\n",
    "    learnX = (learnX - 127.5) / 127.5\n",
    "    \n",
    "    # 生成器と識別器の構築\n",
    "    generator = Generator() # 下のクラスを参照．\n",
    "    discriminator = Discriminator() # 下のクラスを参照．\n",
    "    \n",
    "    # コスト関数と正確度を計算する関数の生成\n",
    "    costComputer = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    accuracyComputer = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # オプティマイザは生成器と識別器で別々のものを利用\n",
    "    optimizerGenerator = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    optimizerDiscriminator = tf.keras.optimizers.Adam(learning_rate=0.00004)\n",
    "    \n",
    "    # 生成器を成長させるためのコストを計算する関数\n",
    "    def generatorCostFunction(discriminatorOutputFromGenerated):\n",
    "        cost = costComputer(tf.ones(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated) # 生成器のコストの引数は生成器の出力を識別器に入れた結果．生成器としては全部正解を出しているはずなので教師はすべて1となるはず．そのように学習すべき．\n",
    "        accuracy = accuracyComputer(tf.ones(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated)\n",
    "        return cost, accuracy\n",
    "    \n",
    "    # 識別器を成長させるためのコストを計算する関数\n",
    "    def discriminatorCostFunction(discriminatorOutputFromReal,discriminatorOutputFromGenerated): # 識別器のコストの引数は本物の情報を識別器に入れた結果と生成器の出力を識別器に入れた結果．\n",
    "        realCost = costComputer(tf.ones(discriminatorOutputFromReal.shape[0]), discriminatorOutputFromReal) # 本物の情報の場合はすべて正例（1）と判断すべき．これが例のコスト関数の左の項に相当．\n",
    "        fakeCost = costComputer(tf.zeros(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated) # 偽物の情報の場合はすべて負例（0）と判断すべき．これが例のコスト関数の右の項に相当．\n",
    "        cost = realCost + fakeCost\n",
    "        realAccuracy = accuracyComputer(tf.ones(discriminatorOutputFromReal.shape[0]), discriminatorOutputFromReal)\n",
    "        fakeAccuracy = accuracyComputer(tf.zeros(discriminatorOutputFromGenerated.shape[0]), discriminatorOutputFromGenerated)\n",
    "        accuracy=(realAccuracy + fakeAccuracy) / 2\n",
    "        return cost, accuracy\n",
    "    \n",
    "    @tf.function()\n",
    "    def run(generator, discriminator, noiseVector, realVector):\n",
    "        with tf.GradientTape() as generatorTape, tf.GradientTape() as discriminatorTape:\n",
    "            generatedVector = generator(noiseVector) # 生成器によるデータの生成．\n",
    "            discriminatorOutputFromGenerated = discriminator(generatedVector) # その生成データを識別器に入れる．\n",
    "            discriminatorOutputFromReal = discriminator(realVector) # 本物データを識別器に入れる．\n",
    "            # 識別器の成長\n",
    "            discriminatorCost, discriminatorAccuracy = discriminatorCostFunction(discriminatorOutputFromReal, discriminatorOutputFromGenerated)\n",
    "            gradientDiscriminator = discriminatorTape.gradient(discriminatorCost, discriminator.trainable_variables) # 識別器のパラメータだけで勾配を計算．つまり生成器のパラメータは行わない．\n",
    "            optimizerDiscriminator.apply_gradients(zip(gradientDiscriminator, discriminator.trainable_variables))\n",
    "            # 生成器の成長\n",
    "            generatorCost, generatorAccuracy = generatorCostFunction(discriminatorOutputFromGenerated)\n",
    "            gradientGenerator = generatorTape.gradient(generatorCost,generator.trainable_variables) # 生成器のパラメータで勾配を計算．\n",
    "            optimizerGenerator.apply_gradients(zip(gradientGenerator,generator.trainable_variables))\n",
    "            return discriminatorCost, discriminatorAccuracy, generatorCost, generatorAccuracy\n",
    "    \n",
    "    # ミニバッチセットの生成\n",
    "    learnX = tf.data.Dataset.from_tensor_slices(learnX) # このような方法を使うと簡単にミニバッチを実装することが可能．\n",
    "    learnT = tf.data.Dataset.from_tensor_slices(learnT)\n",
    "    learnA = tf.data.Dataset.zip((learnX, learnT)).shuffle(60000).batch(MiniBatchSize) # 今回はインプットデータしか使わないけど後にターゲットデータを使う場合があるため．\n",
    "    miniBatchNumber = len(list(learnA.as_numpy_iterator()))\n",
    "    # 学習ループ\n",
    "    for epoch in range(1,MaxEpoch+1):\n",
    "        discriminatorCost, discriminatorAccuracy, generatorCost, generatorAccuracy = 0, 0, 0, 0\n",
    "        for learnx, _ in learnA:\n",
    "            noiseVector = generateNoise(MiniBatchSize, NoiseSize) # ミニバッチサイズで100個の要素からなるノイズベクトルを生成．\n",
    "            discriminatorCostTmp, discriminatorAccuracyTmp, generatorCostTmp, generatorAccuracyTmp = run(generator, discriminator, noiseVector, learnx)\n",
    "            discriminatorCost += discriminatorCostTmp / miniBatchNumber\n",
    "            discriminatorAccuracy += discriminatorAccuracyTmp / miniBatchNumber\n",
    "            generatorCost += generatorCostTmp / miniBatchNumber\n",
    "            generatorAccuracy += generatorAccuracyTmp / miniBatchNumber\n",
    "        # 疑似的なテスト\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch {:10d} D-cost {:6.4f} D-acc {:6.4f} G-cost {:6.4f} G-acc {:6.4f} \".format(epoch,float(discriminatorCost),float(discriminatorAccuracy),float(generatorCost),float(generatorAccuracy)))\n",
    "            validationNoiseVector = generateNoise(1, NoiseSize)\n",
    "            validationOutput = generator(validationNoiseVector)\n",
    "            validationOutput = np.asarray(validationOutput).reshape([1, 28, 28])\n",
    "            plt.imshow(validationOutput[0], cmap = \"gray\")\n",
    "            plt.pause(1)\n",
    "\n",
    "# 入力されたデータを0か1に分類するネットワーク\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(units=128)\n",
    "        self.d2 = tf.keras.layers.Dense(units=128)\n",
    "        self.d3 = tf.keras.layers.Dense(units=128)\n",
    "        self.d4 = tf.keras.layers.Dense(units=2, activation=\"softmax\")\n",
    "        self.a = tf.keras.layers.LeakyReLU()\n",
    "        self.d = tf.keras.layers.Dropout(0.5)\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.d(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.d(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.a(y)\n",
    "        y = self.d(y)\n",
    "        y = self.d4(y)\n",
    "        return y\n",
    "\n",
    "# 入力されたベクトルから別のベクトルを生成するネットワーク\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.d1=tf.keras.layers.Dense(units=256)\n",
    "        self.d2=tf.keras.layers.Dense(units=256)\n",
    "        self.d3=tf.keras.layers.Dense(units=784)\n",
    "        self.a=tf.keras.layers.LeakyReLU()\n",
    "        self.b1=tf.keras.layers.BatchNormalization()\n",
    "        self.b2=tf.keras.layers.BatchNormalization()\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.b1(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.b2(y)\n",
    "        y = self.d3(y)\n",
    "        return y\n",
    "\n",
    "def generateNoise(miniBatchSize, randomNoiseSize):\n",
    "    return np.random.uniform(-1,1,size=(miniBatchSize,randomNoiseSize)).astype(\"float32\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZpHl3AlpmSN"
   },
   "source": [
    "とても簡単に自然言語処理を実現することができる利用方法を紹介します．世界最高性能を求めたいとかでないなら，ここで紹介する方法を利用して様々なことを達成できると思います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saOu-Onk5d-w"
   },
   "source": [
    "### GAN の問題点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_RY-XjT5d-w"
   },
   "source": [
    "最も簡単な `tranformers` の利用方法は以下のようになると思います．`pipeline` を読み込んで，そこに取り組みたいタスク（`sentiment-analysis`）を指定します．初回の起動の際には事前学習済みモデルがダウンロードされるため時間がかかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4QgsbGyfBA3"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    text = \"I have a pen.\"\n",
    "    result = classifier(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4KbHIJHDFk"
   },
   "source": [
    "入力した文章がポジティブな文章なのかネガティブな文章なのかを分類できます．ここでは1個の文章を入力しましたが，以下のように2個以上の文章も入力可能です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVrVwjvFHVax"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    litext = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    "    result = classifier(litext)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqwWY1c0O02R"
   },
   "source": [
    "## WGAN-gp の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pjsr5ACupmST"
   },
   "source": [
    "これまでに利用したものとは異なるモデルを利用したいとか，自身が持っているデータセットにより適合させたいとかの応用的な利用方法を紹介します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-qwkX3SCCRC"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def main():\n",
    "    # ハイパーパラメータの設定\n",
    "    MiniBatchSize = 300\n",
    "    NoiseSize = 100 # GANはランダムなノイズベクトルから何かを生成する方法なので，そのノイズベクトルのサイズを設定する．\n",
    "    MaxEpoch = 300\n",
    "    CriticLearningNumber = 5\n",
    "    GradientPenaltyCoefficient = 10\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    (learnX, learnT), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    learnX = np.asarray(learnX.reshape([60000, 784]), dtype=\"float32\")\n",
    "    learnX = (learnX - 127.5) / 127.5\n",
    "    \n",
    "    # 生成器と識別器の構築\n",
    "    generator = Generator() # 下のクラスを参照．\n",
    "    critic = Critic() # 下のクラスを参照．\n",
    "    \n",
    "    # オプティマイザは生成器と識別器で同じで良い．が，ハイパーパラメータを変えたくなるかもしれないからふたつ用意．\n",
    "    optimizerGenerator = tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0,beta_2=0.9)\n",
    "    optimizerCritic = tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0,beta_2=0.9)\n",
    "    \n",
    "    @tf.function()\n",
    "    def runCritic(generator, critic, noiseVector, realVector):\n",
    "        with tf.GradientTape() as criticTape:\n",
    "            generatedVector = generator(noiseVector) # 生成器によるデータの生成．\n",
    "            criticOutputFromGenerated = critic(generatedVector) # その生成データを識別器に入れる．\n",
    "            criticOutputFromReal = critic(realVector) # 本物データを識別器に入れる．\n",
    "            epsilon = tf.random.uniform(generatedVector.shape, minval=0, maxval=1)\n",
    "            intermediateVector = generatedVector + epsilon * (realVector - generatedVector)\n",
    "            # 勾配ペナルティ\n",
    "            with tf.GradientTape() as gradientPenaltyTape:\n",
    "                gradientPenaltyTape.watch(intermediateVector)\n",
    "                criticOutputFromIntermediate = critic(intermediateVector)\n",
    "                gradientVector = gradientPenaltyTape.gradient(criticOutputFromIntermediate, intermediateVector)\n",
    "                gradientNorm = tf.norm(gradientVector, ord=\"euclidean\", axis=1) # gradientNorm = tf.sqrt(tf.reduce_sum(tf.square(gradientVector), axis=1)) と書いても良い．\n",
    "                gradientPenalty = (gradientNorm - 1)**2\n",
    "            # 識別器の成長\n",
    "            criticCost = tf.reduce_mean(criticOutputFromGenerated - criticOutputFromReal + GradientPenaltyCoefficient * gradientPenalty) # 識別器を成長させるためのコストを計算．WGANの元論文の式そのまま．\n",
    "            gradientCritic = criticTape.gradient(criticCost, critic.trainable_variables) # 識別器のパラメータだけで勾配を計算．つまり生成器のパラメータは行わない．\n",
    "            optimizerCritic.apply_gradients(zip(gradientCritic, critic.trainable_variables))\n",
    "            return criticCost\n",
    "    \n",
    "    @tf.function()\n",
    "    def runGenerator(generator, critic, noiseVector):\n",
    "        with tf.GradientTape() as generatorTape:\n",
    "            generatedVector = generator(noiseVector) # 生成器によるデータの生成．\n",
    "            criticOutputFromGenerated = critic(generatedVector) # その生成データを識別器に入れる．\n",
    "            # 生成器の成長\n",
    "            generatorCost = -tf.reduce_mean(criticOutputFromGenerated) # 生成器を成長させるためのコストを計算．\n",
    "            gradientGenerator = generatorTape.gradient(generatorCost,generator.trainable_variables) # 生成器のパラメータで勾配を計算．\n",
    "            optimizerGenerator.apply_gradients(zip(gradientGenerator,generator.trainable_variables))\n",
    "            return generatorCost\n",
    "    \n",
    "    # ミニバッチセットの生成\n",
    "    learnX = tf.data.Dataset.from_tensor_slices(learnX) # このような方法を使うと簡単にミニバッチを実装することが可能．\n",
    "    learnT = tf.data.Dataset.from_tensor_slices(learnT)\n",
    "    learnA = tf.data.Dataset.zip((learnX, learnT)).shuffle(60000).batch(MiniBatchSize) # 今回はインプットデータしか使わないけど後にターゲットデータを使う場合があるため．\n",
    "    miniBatchNumber = len(list(learnA.as_numpy_iterator()))\n",
    "    # 学習ループ\n",
    "    for epoch in range(1,MaxEpoch+1):\n",
    "        criticCost, generatorCost = 0, 0\n",
    "        for learnx, _ in learnA:\n",
    "            # WGAN-gpでは識別器1回に対して生成器を複数回学習させるのでそのためのループ．\n",
    "            for _ in range(CriticLearningNumber):\n",
    "                noiseVector = generateNoise(MiniBatchSize, NoiseSize) # ミニバッチサイズで100個の要素からなるノイズベクトルを生成．\n",
    "                criticCostPiece = runCritic(generator, critic, noiseVector, learnx)\n",
    "                criticCost += criticCostPiece / (CriticLearningNumber * miniBatchNumber)\n",
    "            # WGAN-gpでは識別器1回に対して生成器を複数回学習させるのでそのためのループ．\n",
    "            for _ in range(1):\n",
    "                noiseVector = generateNoise(MiniBatchSize, NoiseSize) # ミニバッチサイズで100個の要素からなるノイズベクトルを生成．\n",
    "                generatorCostPiece = runGenerator(generator, critic, noiseVector)\n",
    "                generatorCost += generatorCostPiece / miniBatchNumber\n",
    "        # 疑似的なテスト\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch {:10d} D-cost {:6.4f} G-cost {:6.4f} \".format(epoch,float(criticCost),float(generatorCost)))\n",
    "            validationNoiseVector = generateNoise(1, NoiseSize)\n",
    "            validationOutput = generator(validationNoiseVector)\n",
    "            validationOutput = np.asarray(validationOutput).reshape([1, 28, 28])\n",
    "            plt.imshow(validationOutput[0], cmap = \"gray\")\n",
    "            plt.pause(1)\n",
    "\n",
    "# 入力されたデータを0か1に分類するネットワーク\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic,self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(units=128)\n",
    "        self.d2 = tf.keras.layers.Dense(units=128)\n",
    "        self.d3 = tf.keras.layers.Dense(units=128)\n",
    "        self.d4 = tf.keras.layers.Dense(units=1)\n",
    "        self.a = tf.keras.layers.LeakyReLU()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.a(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.d4(y)\n",
    "        return y\n",
    "\n",
    "# 入力されたベクトルから別のベクトルを生成するネットワーク\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.d1=tf.keras.layers.Dense(units=128)\n",
    "        self.d2=tf.keras.layers.Dense(units=128)\n",
    "        self.d3=tf.keras.layers.Dense(units=128)\n",
    "        self.d4=tf.keras.layers.Dense(units=784)\n",
    "        self.a=tf.keras.layers.LeakyReLU()\n",
    "        self.b1=tf.keras.layers.BatchNormalization()\n",
    "        self.b2=tf.keras.layers.BatchNormalization()\n",
    "        self.b3=tf.keras.layers.BatchNormalization()\n",
    "    def call(self,x):\n",
    "        y = self.d1(x)\n",
    "        y = self.a(y)\n",
    "        y = self.b1(y)\n",
    "        y = self.d2(y)\n",
    "        y = self.a(y)\n",
    "        y = self.b2(y)\n",
    "        y = self.d3(y)\n",
    "        y = self.a(y)\n",
    "        y = self.b3(y)\n",
    "        y = self.d4(y)\n",
    "        y = tf.keras.activations.tanh(y)\n",
    "        return y\n",
    "\n",
    "def generateNoise(miniBatchSize, randomNoiseSize):\n",
    "    return np.random.uniform(-1,1,size=(miniBatchSize,randomNoiseSize)).astype(\"float32\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSKzyoA1O02W"
   },
   "source": [
    "### WGAN-gp とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G316vauoido7"
   },
   "source": [
    "これまでに，Hugging Face が自動でダウンロードしてくれたデフォルトの事前学習済みモデルを利用した予測を行いましたが，そうでなくて，モデルを指定することもできます．以下のページをご覧ください．Model Hub と言います．\n",
    "\n",
    "https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "\n",
    "\n",
    "この Model Hub の Tasks というところでタグを選択できます．例えば，Text Generation の `distilgpt2` を利用するには以下のように書きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEduyYGdPhf0"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from transformers import pipeline\n",
    " \n",
    "def main():\n",
    "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    text = \"In this course, we will teach you how to\"\n",
    "    result = generator(text)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R2Wn48-4NwS"
   },
   "source": [
    "### WGAN-gp の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD4N6qje9TwB"
   },
   "source": [
    "## CGAN の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCIkDQK74Q2_"
   },
   "source": [
    "### CGAN とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaJRhYid4FOV"
   },
   "source": [
    "<img src=\"https://github.com/yamada-kd/binds-training/blob/main/image/cgan.svg?raw=1\" width=\"100%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBMr-6Sp4Te_"
   },
   "source": [
    "### CGAN の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfC8Kj03omW"
   },
   "source": [
    "```{note}\n",
    "終わりです．\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_8.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
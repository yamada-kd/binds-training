{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_xhvUO10XD6"
   },
   "source": [
    "# 教師なし学習法（編集中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZQz79CS4WfU"
   },
   "source": [
    "## 教師なし学習法の種類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h6iDCxGOk2p"
   },
   "source": [
    "教師なし学習法には様々な種類があります．https://scikit-learn.org/stable/unsupervised_learning.html にはそれらがまとめられています．リンク先のページには以下に示すものが列挙されています．\n",
    "\n",
    "*   混合ガウスモデル\n",
    "*   多様体学習\n",
    "*   クラスタリング\n",
    "*   バイクラスタリング\n",
    "*   行列分解\n",
    "*   共分散推定\n",
    "*   外れ値検知\n",
    "*   密度推定\n",
    "*   制約付きボルツマンマシン\n",
    "\n",
    "中でも代表的なものには，階層的クラスタリング法，非階層的クラスタリング法（特に K-means），主成分分析法，t-SNE，カーネル密度推定法，自己組織化マップ，敵対的生成ネットワークがあります．教師なし学習法は主に与えられたデータの性質を理解するために利用されます．与えられたデータの中で類似しているインスタンスを集めるとか，与えられたデータの関係性を人間が理解しやすい方法（次元削減）で可視化するとかです．また，敵対的生成ネットワークはこれらとは異なり特殊な機械学習アルゴリズムで，新たなデータを生成するために利用されます．このコンテンツでは教師なし学習法の中でも scikit-learn を利用して簡単に実装できる最も代表的な手法の使い方を紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mAPiZjtfkWx"
   },
   "source": [
    "```{note}\n",
    "敵対的生成ネットワークは別に紹介します．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcnoJlnfOk2r"
   },
   "source": [
    "次の節では scikit-learn を利用して，階層的クラスタリング法，K-means 法（非階層的クラスタリング法の代表的な手法），主成分分析法，カーネル密度推定法を実装します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JIH6ZHaOk2s"
   },
   "source": [
    "## 階層的クラスタリング法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRbtig5lOk2s"
   },
   "source": [
    "ほげ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdFQn5noOk2s"
   },
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT9agRvoOk2t"
   },
   "source": [
    "ほげ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cwUb15zOk2t"
   },
   "source": [
    "## 主成分分析法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFRSh4DUOk2t"
   },
   "source": [
    "ほげ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxolXhqEOk2t"
   },
   "source": [
    "## カーネル密度推定法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZTDZ9lperCn"
   },
   "source": [
    "### 基本的な事柄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Mh-KoTOk2t"
   },
   "source": [
    "カーネル密度推定法は与えられたデータの分布を推定する方法です．与えられたデータ中の疎なインスタンスを入力としてそのデータが従うと思われる分布を推定する方法です．$x_1, x_2, \\dots, x_n$ を何らかの確率分布から得られたサンプルとします．このときにカーネル密度推定量 $f$ は以下のように計算されます．\n",
    "\n",
    "$\n",
    "\\displaystyle f(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\left(\\frac{x-x_i}{h}\\right)\n",
    "$\n",
    "\n",
    "このとき，$K$ はカーネル関数と呼ばれる確率分布を近似するための関数で，$h$ はバンド幅と呼ばれるハイパーパラメータです．カーネル関数として利用される関数には様々なものがありますが，以下の標準正規分布（平均値が0で分散が1である正規分布）の確率密度関数を利用することが多いです．\n",
    "\n",
    "$\n",
    "\\displaystyle K(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ShGJD1icm9z"
   },
   "source": [
    "```{note}\n",
    "このカーネルのことはガウシアンカーネルとも言いますね．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqZ5v9xgeuH8"
   },
   "source": [
    "### 発展的な利用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXpe9scXfcjW"
   },
   "source": [
    "昨今の深層学習ブームで専ら利用されている深層ニューラルネットワークは通常たくさんのハイパーパラメータを持ちます．良いハイパーパラメータを同定することは良い人工知能を構築するために重要なことであるため，その探索方法の開発が活発です．ブルートフォース（しらみつぶし）な探索，ランダムな探索，手動による探索，進化戦略法を利用した探索，ベイズ探索等の様々な方法が利用されていますが，その中でも最も有効なもののひとつに代理モデルを利用した逐次最適化法（sequential model-based optimization（SMBO））と呼ばれるベイズ最適化法の範疇に入る方法があります．ハイパーパラメータが従うであろう分布を推定して（この推定した分布を代理モデルと言います），その分布から予想したハイパーパラメータを利用して構築した人工知能の評価を行い，さらにその評価結果から分布の推定を繰り返す，というようなハイパーパラメータの従う分布の推定と人工知能の評価を交互に繰り返すことで最適なハイパーパラメータを持つ人工知能を同定しようとする方法です．この SMBO を行う際の代理モデルの構築にカーネル密度推定法が利用されることがあります．そして，カーネル密度推定法を利用した SMBO は従来の代理モデルの推定法（例えば，ガウス過程回帰法）より良い性能を示すことがあります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnrjFTUaiDku"
   },
   "source": [
    "```{note}\n",
    "SMBO の領域ではカーネル密度推定量はパルツェン推定量と呼ばれています．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVkVj79MiSMH"
   },
   "source": [
    "```{note}\n",
    "ハイパーパラメータの最適化は深層学習の分野で最も重要なトピックのひとつなので紹介してみました．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA7TKdR4e6gk"
   },
   "source": [
    "### 元の確率分布の推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlaLyet4fB4U"
   },
   "source": [
    "ここでは，母数の異なるふたつの正規分布からいくつかのインスタンスをサンプリングして，そのサンプリングしたデータから元の正規分布ふたつからなる二峰性の確率分布を再現できるかということを試します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1txwtm2K1oX-"
   },
   "source": [
    "```{hint}\n",
    "正規分布の母数（パラメータ）は平均値と分散ですね．母数の値が決まればそれに対応する正規分布の形状は一意に決まるのですね．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERhAr5LjSau5"
   },
   "source": [
    "以下のコードで $N(-3, 1.5)$ と $N(-3, 2)$ の正規分布を描画します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqrlx5Qj2ZGR"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    " \n",
    "def main():\n",
    "    x = np.linspace(-8, 8, 100)\n",
    "    y = (norm.pdf(x, loc=2, scale=1.5) + norm.pdf(x, loc=-3, scale=2)) / 2\n",
    "    plt.plot(x, y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11x87fk-S6r_"
   },
   "source": [
    "次に，以下のコードで $N(-3, 1.5)$ に従う50個のインスタンスと $N(-3, 2)$ に従う50個のインスタンスをサンプリングします．また，そのヒストグラムを描きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3ejvHBDTFeY"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "np.random.seed(0)\n",
    " \n",
    "def main():\n",
    "    x1 = norm.rvs(loc=2, scale=1.5, size=50)\n",
    "    plt.hist(x1)\n",
    "    x2 = norm.rvs(loc=-3, scale=2, size=50)\n",
    "    plt.hist(x2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP57wd1MTtrc"
   },
   "source": [
    "```{attention}\n",
    "計算機実験をする際は乱数の種は固定しなきゃならないのでしたね．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueNQ6kYSb36Y"
   },
   "source": [
    "カーネル密度推定は以下のように行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JHa8o_0Ul3W"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    " \n",
    "def main():\n",
    "    x1 = norm.rvs(loc=2, scale=1.5, size=1000)\n",
    "    x2 = norm.rvs(loc=-3, scale=2, size=1000)\n",
    "    x = np.concatenate([x1, x2]) # x1とx2を連結します\n",
    "    x = x.reshape(-1, 1) # このような入力形式にしないと受け付けてくれないからこうしました．\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.4).fit(x) # ハンド幅は適当に選んでみました．\n",
    "    p = np.linspace(-8, 8, 100)[:, np.newaxis] # プロット用の値を生成しています．\n",
    "    l = kde.score_samples(p) # これで予測値を計算します．\n",
    "    plt.plot(p, np.exp(l)) # 予測値は対数値で出力されているのでそれをnp.exp()を利用してプロットします．\n",
    "    y = (norm.pdf(p, loc=2, scale=1.5) + norm.pdf(p, loc=-3, scale=2)) / 2 # 元の分布です．\n",
    "    plt.plot(p, y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U92DvL3cb7PD"
   },
   "source": [
    "```{note}\n",
    "サンプリングしたインスタンスを使って予測した分布の形状と元の分布の形状が類似している様子がわかります．\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6pr7p42fCTx"
   },
   "source": [
    "### 画像の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zu4ZCq2xRfab"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "def main():\n",
    "    diris = load_iris()\n",
    "    learnx, testx, learnt, testt = train_test_split(diris.data, diris.target, test_size = 0.2, random_state = 0)\n",
    "    predictor = DecisionTreeClassifier(random_state=0) # 予測器を生成．ここも乱数の種に注意．\n",
    "    predictor.fit(learnx, learnt) # 学習．\n",
    "    print(predictor.predict(testx)) # テストデータセットの入力データを予測器に入れて結果を予測．\n",
    "    print(testt) # 教師データ．\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rfC8Kj03omW"
   },
   "source": [
    "```{note}\n",
    "終わりです．\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "scikit-learn-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}